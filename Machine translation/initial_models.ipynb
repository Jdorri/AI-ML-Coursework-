{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ffnn_sklearn.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JPV_05dyqE5",
        "colab_type": "text"
      },
      "source": [
        "# Embeddings 100/300 dims"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVStPp2J6Fmf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "def writeScores(scores, fn):\n",
        "    with open(fn, 'w') as output_file:\n",
        "        for idx,x in enumerate(scores):\n",
        "            output_file.write(f\"{x}\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5y34iNipyr3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from os.path import exists\n",
        "\n",
        "if not exists('enzh_data.zip'):\n",
        "    !wget -O enzh_data.zip https://competitions.codalab.org/my/datasets/download/03e23bd7-8084-4542-997b-6a1ca6dd8a5f\n",
        "    !unzip enzh_data.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlXMiqJXq8fy",
        "colab_type": "code",
        "outputId": "149cbb76-1817-4f2c-b382-26e4f8e79adb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "#English-Chinese\n",
        "#Checking Data\n",
        "print(\"---EN-ZH---\")\n",
        "print()\n",
        "\n",
        "with open(\"./train.enzh.src\", \"r\") as enzh_src:\n",
        "  print(\"Source: \",enzh_src.readline())\n",
        "with open(\"./train.enzh.mt\", \"r\") as enzh_mt:\n",
        "  print(\"Translation: \",enzh_mt.readline())\n",
        "with open(\"./train.enzh.scores\", \"r\") as enzh_scores:\n",
        "  print(\"Score: \",enzh_scores.readline())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---EN-ZH---\n",
            "\n",
            "Source:  The last conquistador then rides on with his sword drawn.\n",
            "\n",
            "Translation:  最后的征服者骑着他的剑继续前进.\n",
            "\n",
            "Score:  -1.5284005772625449\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lc4rdJnrE_Q",
        "colab_type": "code",
        "outputId": "3a6dd00b-c79e-4f71-a90e-6b3783ae78cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "# DON'T RUN IF YOU ALREADY RAN IT IN THE ENGLISH-GERMAN SECTION\n",
        "# Downloading spacy models for english\n",
        "\n",
        "!spacy download en_core_web_md\n",
        "!spacy link en_core_web_md en300"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_md==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.1.0/en_core_web_md-2.1.0.tar.gz#egg=en_core_web_md==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n",
            "\n",
            "\u001b[38;5;1m✘ Link 'en300' already exists\u001b[0m\n",
            "To overwrite an existing link, use the --force flag\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fx3Ja9zWFDj2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchtext\n",
        "import spacy\n",
        "\n",
        "#dim=100 Embeddings\n",
        "glove = torchtext.vocab.GloVe(name='6B', dim=100)\n",
        "\n",
        "#dim=300 Embeddings\n",
        "#glove = torchtext.vocab.GloVe(name='6B', dim=300)\n",
        "\n",
        "#tokenizer model\n",
        "nlp_en =spacy.load('en300')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BUi2QiCIi9y",
        "colab_type": "code",
        "outputId": "064352d1-bc92-4f98-cafc-3d2dafce3d2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#ENGLISH EMBEDDINGS methods from the section GERMAN-ENGLISH\n",
        "# The difference from previous section is that we will use Glove embeddings directly because we are using a smaller model that spacy doesn't have\n",
        "# We add a method to compute the word embedding and a method to compute the sentence embedding by averaging the word vectors\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from nltk import download\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "#downloading stopwords from the nltk package\n",
        "download('stopwords') #stopwords dictionary, run once\n",
        "stop_words_en = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "def preprocess(sentence,nlp):\n",
        "    text = sentence.lower()\n",
        "    doc = [token.lemma_ for token in  nlp.tokenizer(text)]\n",
        "    #doc = [word for word in doc if word not in stop_words_en]\n",
        "    doc = [word for word in doc if word.isalpha()] #restricts string to alphabetic characters only\n",
        "    return doc\n",
        "\n",
        "def get_word_vector(embeddings, word):\n",
        "    try:\n",
        "      vec = embeddings.vectors[embeddings.stoi[word]]\n",
        "      return vec\n",
        "    except KeyError:\n",
        "      #print(f\"Word {word} does not exist\")\n",
        "      pass\n",
        "\n",
        "def get_sentence_mean_vector(embeddings,line):\n",
        "  vectors = []\n",
        "  for w in line:\n",
        "    emb = get_word_vector(embeddings,w)\n",
        "    #do not add if the word is out of vocabulary\n",
        "    if emb is not None:\n",
        "      vectors.append(emb)\n",
        "   \n",
        "  return torch.mean(torch.stack(vectors))\n",
        "\n",
        "\n",
        "def get_sentence_vector(embeddings,line):\n",
        "  vectors = []\n",
        "  for w in line:\n",
        "    emb = get_word_vector(embeddings,w)\n",
        "    #do not add if the word is out of vocabulary\n",
        "    if emb is not None:\n",
        "      vectors.append(emb)\n",
        "   \n",
        "  return torch.stack(vectors)\n",
        "\n",
        "\n",
        "def get_embeddings(f,embeddings,lang):\n",
        "  file = open(f) \n",
        "  lines = file.readlines() \n",
        "  sentences_vectors =[]\n",
        "\n",
        "  for l in lines:\n",
        "    sentence= preprocess(l,lang)\n",
        "    try:\n",
        "      vec = get_sentence_vector(embeddings,sentence)\n",
        "      sentences_vectors.append(vec)\n",
        "    except:\n",
        "      sentences_vectors.append(0)\n",
        "\n",
        "  return sentences_vectors\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jW3S2-rs6BV",
        "colab_type": "code",
        "outputId": "87b70c03-3cc6-43de-bf6c-7993ba4cfc35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "\n",
        "!wget -c https://github.com/Tony607/Chinese_sentiment_analysis/blob/master/data/chinese_stop_words.txt\n",
        "\n",
        "!wget -O zh.zip http://vectors.nlpl.eu/repository/20/35.zip\n",
        "\n",
        "!unzip zh.zip \n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-28 14:30:34--  https://github.com/Tony607/Chinese_sentiment_analysis/blob/master/data/chinese_stop_words.txt\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘chinese_stop_words.txt’\n",
            "\n",
            "chinese_stop_words.     [  <=>               ] 417.16K  1.22MB/s    in 0.3s    \n",
            "\n",
            "2020-02-28 14:30:35 (1.22 MB/s) - ‘chinese_stop_words.txt’ saved [427175]\n",
            "\n",
            "--2020-02-28 14:30:36--  http://vectors.nlpl.eu/repository/20/35.zip\n",
            "Resolving vectors.nlpl.eu (vectors.nlpl.eu)... 129.240.189.225\n",
            "Connecting to vectors.nlpl.eu (vectors.nlpl.eu)|129.240.189.225|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1458485917 (1.4G) [application/zip]\n",
            "Saving to: ‘zh.zip’\n",
            "\n",
            "zh.zip              100%[===================>]   1.36G  16.8MB/s    in 90s     \n",
            "\n",
            "2020-02-28 14:32:06 (15.5 MB/s) - ‘zh.zip’ saved [1458485917/1458485917]\n",
            "\n",
            "Archive:  zh.zip\n",
            "  inflating: LIST                    \n",
            "  inflating: meta.json               \n",
            "  inflating: model.bin               \n",
            "  inflating: model.txt               \n",
            "  inflating: README                  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P69wJb4JPM27",
        "colab_type": "code",
        "outputId": "440731c6-1dec-4bb8-bbc2-7cc3c8618a4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import files, drive\n",
        "%matplotlib inline\n",
        "\n",
        "# Mount your google drive to get the data from\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGsrUZBMy_5X",
        "colab_type": "code",
        "outputId": "a6843f8f-03d8-49a3-c4bd-7a39e8fbaa12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "#dim=300\n",
        "!unzip '/content/gdrive/My Drive/Colab Notebooks/zh.zip'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/gdrive/My Drive/Colab Notebooks/zh.zip\n",
            "  inflating: zh.bin                  \n",
            "  inflating: zh.tsv                  \n",
            "  inflating: zh.bin.syn1neg.npy      \n",
            "  inflating: zh.bin.syn0.npy         \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDUbXQ4aMv1K",
        "colab_type": "code",
        "outputId": "09c5776a-992d-4c88-9fc4-a5c684bdd03f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "#dim=100\n",
        "wv_from_bin = KeyedVectors.load_word2vec_format(\"model.bin\", binary=True)\n",
        "\n",
        "#dim=300\n",
        "#wv_from_bin = Word2Vec.load('zh.bin')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LA9N1zgsSQl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "import jieba\n",
        "import gensim \n",
        "import spacy\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "stop_words = [ line.rstrip() for line in open('./chinese_stop_words.txt',\"r\", encoding=\"utf-8\") ]\n",
        "\n",
        "\n",
        "def get_sentence_vector_mean_zh(line):\n",
        "  vectors = []\n",
        "  for w in line:\n",
        "    try:\n",
        "      emb = wv_from_bin[w]\n",
        "      vectors.append(emb)\n",
        "    except:\n",
        "      pass #Do not add if the word is out of vocabulary\n",
        "  if vectors:\n",
        "    vectors = np.array(vectors)\n",
        "    return np.mean(vectors)  \n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "\n",
        "def get_sentence_vector_zh(line):\n",
        "  vectors = []\n",
        "  for w in line:\n",
        "    try:\n",
        "      emb = wv_from_bin[w]\n",
        "      emb = torch.from_numpy(emb)\n",
        "      vectors.append(emb)\n",
        "    except:\n",
        "      vectors.append(torch.zeros(100))\n",
        "      #vectors.append(torch.zeros(300))\n",
        "  vectors = torch.stack(vectors)\n",
        "  return vectors  \n",
        "\n",
        "    \n",
        "\n",
        "def processing_zh(sentence):\n",
        "  seg_list = jieba.lcut(sentence,cut_all=True)\n",
        "  #doc = [word for word in seg_list if word not in stop_words]\n",
        "  doc = [word for word in seg_list]\n",
        "  docs = [e for e in doc if e.isalnum()]\n",
        "  return docs\n",
        "\n",
        "\n",
        "def get_sentence_embeddings_zh(f):\n",
        "  file = open(f) \n",
        "  lines = file.readlines() \n",
        "  sentences_vectors =[]\n",
        "  for l in lines:\n",
        "    sent  = processing_zh(l)\n",
        "    vec = get_sentence_vector_zh(sent)\n",
        "\n",
        "    if vec is not None:\n",
        "      sentences_vectors.append(vec)\n",
        "    else:\n",
        "      print(l)\n",
        "  return sentences_vectors\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zVjor64tR8D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "d2c88bb0-6636-4364-a692-383eba0435e4"
      },
      "source": [
        "import spacy\n",
        "import torchtext\n",
        "from torchtext import data\n",
        "\n",
        "\n",
        "zh_train_mt = get_sentence_embeddings_zh(\"./train.enzh.mt\")\n",
        "zh_train_src = get_embeddings(\"./train.enzh.src\",glove,nlp_en)\n",
        "f_train_scores = open(\"./train.enzh.scores\",'r')\n",
        "zh_train_scores = f_train_scores.readlines()\n",
        "\n",
        "\n",
        "zh_val_mt = get_sentence_embeddings_zh(\"./dev.enzh.mt\")\n",
        "zh_val_src = get_embeddings(\"./dev.enzh.src\",glove,nlp_en)\n",
        "f_val_scores = open(\"./dev.enzh.scores\",'r')\n",
        "zh_val_scores = f_val_scores.readlines()\n",
        "\n",
        "zh_train_val_mt = zh_train_mt + zh_val_mt\n",
        "zh_train_val_src = zh_train_src + zh_val_src\n",
        "zh_train_val_scores = zh_train_scores + zh_val_scores\n",
        "\n",
        "zh_test_mt = get_sentence_embeddings_zh(\"./test.enzh.mt\")\n",
        "zh_test_src = get_embeddings(\"./test.enzh.src\", glove, nlp_en)\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 0.960 seconds.\n",
            "Prefix dict has been built successfully.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2oU1c-Kaj-0",
        "colab_type": "code",
        "outputId": "ec4a8309-ba22-4ea1-f249-b34313eba393",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(len(zh_train_val_mt))\n",
        "print(zh_train_val_mt[0].shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8000\n",
            "torch.Size([12, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYlzz7uoJaQt",
        "colab_type": "text"
      },
      "source": [
        "## Simple Feed-forward Neural Network model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2sTTDbCkEov",
        "colab_type": "code",
        "outputId": "8cde68c1-c554-46ea-d9e3-14a1d449a79f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "mean_train_src = [torch.mean(sentence, 0) for sentence in zh_train_src]\n",
        "mean_train_mt = [torch.mean(sentence, 0) for sentence in zh_train_mt]\n",
        "mean_train = [torch.cat(tensor_pair) for tensor_pair in zip(mean_train_src, mean_train_mt)]\n",
        "\n",
        "train_scores = np.array(zh_train_scores).astype(float)\n",
        "\n",
        "mean_val_src = [torch.mean(sentence, 0) for sentence in zh_val_src]\n",
        "mean_val_mt = [torch.mean(sentence, 0) for sentence in zh_val_mt]\n",
        "mean_val = [torch.cat(tensor_pair) for tensor_pair in zip(mean_val_src, mean_val_mt)]\n",
        "\n",
        "val_scores = np.array(zh_val_scores).astype(float)\n",
        "\n",
        "mean_train_val_src = [torch.mean(sentence, 0) for sentence in zh_train_val_src]\n",
        "mean_train_val_mt = [torch.mean(sentence, 0) for sentence in zh_train_val_mt]\n",
        "mean_train_val = [torch.cat(tensor_pair) for tensor_pair in zip(mean_train_val_src, mean_train_val_mt)]\n",
        "\n",
        "train_val_scores = np.array(zh_train_val_scores).astype(float)\n",
        "\n",
        "mean_test_src = [torch.mean(sentence, 0) for sentence in zh_test_src]\n",
        "mean_test_mt = [torch.mean(sentence, 0) for sentence in zh_test_mt]\n",
        "mean_test = [torch.cat(tensor_pair) for tensor_pair in zip(mean_test_src, mean_test_mt)]\n",
        "\n",
        "print(mean_train[0])\n",
        "print(mean_train[0].shape)\n",
        "print(train_scores)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([-0.0971, -0.1589,  0.2974, -0.1212,  0.1301,  0.3532, -0.0863,  0.3983,\n",
            "        -0.4788,  0.0355,  0.1081,  0.0569,  0.1406, -0.0605,  0.1998, -0.0047,\n",
            "         0.2876, -0.1321, -0.3366, -0.0536,  0.4954, -0.3619,  0.0776,  0.0929,\n",
            "         0.5494, -0.0104, -0.3992, -0.2867,  0.1072, -0.3325, -0.2420,  0.3146,\n",
            "        -0.1398,  0.1598, -0.0720,  0.2631, -0.2362,  0.2532, -0.0221, -0.0299,\n",
            "        -0.2158, -0.2325,  0.3343, -0.3234, -0.0130,  0.0079,  0.1050, -0.2598,\n",
            "         0.1370, -0.4183, -0.2272,  0.1312,  0.2082,  1.1499, -0.1149, -2.3472,\n",
            "        -0.1731, -0.0631,  1.2857,  0.4795, -0.0464,  0.8244, -0.1479,  0.1270,\n",
            "         0.3285, -0.0874,  0.0932,  0.3004, -0.0557, -0.0512, -0.1607, -0.0959,\n",
            "         0.0359, -0.3896, -0.0448,  0.1520, -0.2362, -0.0675, -0.6403,  0.0127,\n",
            "         0.6615,  0.0997, -0.4177,  0.1539, -0.7382, -0.3118, -0.1255, -0.1064,\n",
            "        -0.1151, -0.4154, -0.2467, -0.2316, -0.1042,  0.3197, -0.3741, -0.2296,\n",
            "        -0.3258, -0.0690,  0.4570,  0.0331, -0.0312, -0.3531,  0.1212, -0.0923,\n",
            "        -0.2790,  0.1016,  0.4025,  0.0566, -0.1518,  0.1966,  0.0904,  0.1508,\n",
            "        -0.1913, -0.1448, -0.2264,  0.1023,  0.1945,  0.1411,  0.1837,  0.1115,\n",
            "        -0.3437,  0.0729,  0.3801, -0.2446,  0.0089, -0.0706,  0.2137,  0.4597,\n",
            "         0.4019, -0.1816,  0.0808,  0.1635, -0.0474,  0.1177,  0.1117,  0.1943,\n",
            "        -0.0488, -0.1960, -0.0356, -0.0868,  0.3116,  0.2658,  0.0228,  0.0041,\n",
            "        -0.0654, -0.1821, -0.3420,  0.1124, -0.0613, -0.4044,  0.0522, -0.3403,\n",
            "         0.0314, -0.1120, -0.2194, -0.1507,  0.1640,  0.1864,  0.3292, -0.4026,\n",
            "        -0.1635,  0.5069, -0.1566, -0.1967, -0.0432,  0.3414, -0.2135, -0.2213,\n",
            "         0.0817,  0.0389, -0.3075, -0.0444,  0.0908, -0.0966,  0.1426,  0.1103,\n",
            "        -0.1523,  0.3252, -0.0673, -0.1403,  0.2096, -0.0251, -0.0576,  0.1840,\n",
            "         0.0454, -0.1303,  0.4485,  0.0780,  0.2796, -0.2220, -0.0578, -0.0739,\n",
            "        -0.0410, -0.2482, -0.3660,  0.3485,  0.0594,  0.2088, -0.2307, -0.0441])\n",
            "torch.Size([200])\n",
            "[-1.52840058 -1.05253812  0.42568098 ...  0.11147043  0.97399691\n",
            " -1.97566227]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0FFGfdXqFBu",
        "colab_type": "code",
        "outputId": "ab0b5535-eec6-4a4d-d382-801c27d07762",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(np.max(train_scores))\n",
        "print(np.min(train_scores))\n",
        "\n",
        "print(np.max(val_scores))\n",
        "print(np.min(val_scores))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.5053460474460045\n",
            "-4.704407511656437\n",
            "1.3631834314516038\n",
            "-4.494291459817248\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8o3lWWDEyLt1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import scipy\n",
        "\n",
        "class FFNN(nn.Module):\n",
        "    def __init__(self, hidden_dim, num_classes=1, input_dim=200):  \n",
        "        super(FFNN, self).__init__()\n",
        "        \n",
        "        # embedding (lookup layer) layer\n",
        "        # padding_idx argument makes sure that the 0-th token in the vocabulary\n",
        "        # is used for padding purposes i.e. its embedding will be a 0-vector\n",
        "        # self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        \n",
        "        # hidden layer\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        #self.fc2 = nn.Linear(hidden_dim, 10)\n",
        "        \n",
        "        # activation\n",
        "        self.act1 = nn.ReLU()\n",
        "        #self.act2 = nn.ReLU()\n",
        "\n",
        "\n",
        "        # output layer\n",
        "        self.out = nn.Linear(hidden_dim, num_classes)\n",
        "        self.tanh = nn.Tanh()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # x has shape (1, 2)\n",
        "\n",
        "        out = self.fc1(x)\n",
        "        out = self.act1(out)\n",
        "        #out = self.fc2(out)\n",
        "        #out = self.act2(out)\n",
        "\n",
        "        out = self.out(out)\n",
        "        out = self.tanh(out)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZtN7fhd8cJh",
        "colab_type": "code",
        "outputId": "fb0816b5-509c-44e8-9e27-6629bb4e177d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "mean_train_t = torch.stack(mean_train)\n",
        "mean_val_t = torch.stack(mean_val)\n",
        "mean_train_val_t = torch.stack(mean_train_val)\n",
        "mean_test_t = torch.stack(mean_test)\n",
        "\n",
        "train_scores_t = torch.Tensor(train_scores)\n",
        "val_scores_t = torch.Tensor(val_scores)\n",
        "train_val_scores_t = torch.Tensor(train_val_scores)\n",
        "\n",
        "# Input and label tensors\n",
        "train_dataset = TensorDataset(mean_train_t, train_scores_t)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32)\n",
        "\n",
        "print(train_loader)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<torch.utils.data.dataloader.DataLoader object at 0x7f4d7e87e2b0>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8Lmfe336bEP",
        "colab_type": "code",
        "outputId": "dcba7805-326a-4482-f099-6e432975862e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# we will train for N epochs (The model will see the corpus N times)\n",
        "EPOCHS = 100\n",
        "\n",
        "# Learning rate is initially set to 0.5\n",
        "LRATE = 0.01\n",
        "\n",
        "# dimensionality of the output of the second hidden layer\n",
        "HIDDEN_DIM = 100\n",
        "\n",
        "# Construct the model\n",
        "model = FFNN(HIDDEN_DIM)\n",
        "\n",
        "# Print the model\n",
        "print(model)\n",
        "\n",
        "# we use the stochastic gradient descent (SGD) optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=LRATE)\n",
        "\n",
        "# we use the binary cross-entropy loss with sigmoid (applied to logits) \n",
        "# Recall that we did not apply any activation to our output layer, hence we need\n",
        "# to make our outputs look like probabilities.\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "################\n",
        "# Start training\n",
        "################\n",
        "print(f'Will train for {EPOCHS} epochs')\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "  for batch_idx, (feature, target) in enumerate(train_loader):\n",
        "    # to ensure the dropout (explained later) is \"turned on\" while training\n",
        "    # good practice to include even if do not use here\n",
        "    model.train()\n",
        "  \n",
        "    # we zero the gradients as they are not removed automatically\n",
        "    optimizer.zero_grad()\n",
        "  \n",
        "    # squeeze is needed as the predictions will have the shape (batch size, 1)\n",
        "    # and we need to remove the dimension of size 1\n",
        "    predictions = model(feature).squeeze(1)\n",
        "\n",
        "    # Compute the loss\n",
        "    loss = loss_fn(predictions, target)\n",
        "    train_loss = loss.item()\n",
        "\n",
        "    # calculate the gradient of each parameter\n",
        "    loss.backward()\n",
        "\n",
        "    # update the parameters using the gradients and optimizer algorithm \n",
        "    optimizer.step()\n",
        "  \n",
        "  # this puts the model in \"evaluation mode\" (turns off dropout and batch normalization)\n",
        "  # good practise to include even if we do not use them right now\n",
        "  model.eval()\n",
        "\n",
        "  # we do not compute gradients within this block, i.e. no training\n",
        "  with torch.no_grad():\n",
        "    predictions_valid = model(mean_val_t).squeeze(1)\n",
        "    valid_loss = loss_fn(predictions_valid, val_scores_t).item()\n",
        "    valid_pearson = scipy.stats.pearsonr(predictions_valid, val_scores_t)[0]\n",
        "\n",
        "    predictions_train = model(mean_train_t).squeeze(1)\n",
        "    train_pearson = scipy.stats.pearsonr(predictions_train, train_scores_t)[0]\n",
        "  \n",
        "  print(f'| Epoch: {epoch:02} | Train Loss: {train_loss:.3f} | Val. Loss: {valid_loss:.3f} | Train Pearson: {train_pearson:.3f} | Val. Pearson: {valid_pearson:.3f}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FFNN(\n",
            "  (fc1): Linear(in_features=200, out_features=100, bias=True)\n",
            "  (act1): ReLU()\n",
            "  (out): Linear(in_features=100, out_features=1, bias=True)\n",
            "  (tanh): Tanh()\n",
            ")\n",
            "Will train for 100 epochs\n",
            "| Epoch: 01 | Train Loss: 1.340 | Val. Loss: 0.814 | Train Pearson: 0.225 | Val. Pearson: 0.168\n",
            "| Epoch: 02 | Train Loss: 1.300 | Val. Loss: 0.802 | Train Pearson: 0.248 | Val. Pearson: 0.200\n",
            "| Epoch: 03 | Train Loss: 1.267 | Val. Loss: 0.793 | Train Pearson: 0.257 | Val. Pearson: 0.216\n",
            "| Epoch: 04 | Train Loss: 1.243 | Val. Loss: 0.787 | Train Pearson: 0.266 | Val. Pearson: 0.227\n",
            "| Epoch: 05 | Train Loss: 1.228 | Val. Loss: 0.783 | Train Pearson: 0.275 | Val. Pearson: 0.238\n",
            "| Epoch: 06 | Train Loss: 1.219 | Val. Loss: 0.780 | Train Pearson: 0.284 | Val. Pearson: 0.248\n",
            "| Epoch: 07 | Train Loss: 1.213 | Val. Loss: 0.776 | Train Pearson: 0.292 | Val. Pearson: 0.258\n",
            "| Epoch: 08 | Train Loss: 1.208 | Val. Loss: 0.774 | Train Pearson: 0.299 | Val. Pearson: 0.266\n",
            "| Epoch: 09 | Train Loss: 1.203 | Val. Loss: 0.772 | Train Pearson: 0.306 | Val. Pearson: 0.273\n",
            "| Epoch: 10 | Train Loss: 1.199 | Val. Loss: 0.770 | Train Pearson: 0.312 | Val. Pearson: 0.279\n",
            "| Epoch: 11 | Train Loss: 1.195 | Val. Loss: 0.768 | Train Pearson: 0.317 | Val. Pearson: 0.283\n",
            "| Epoch: 12 | Train Loss: 1.190 | Val. Loss: 0.767 | Train Pearson: 0.321 | Val. Pearson: 0.287\n",
            "| Epoch: 13 | Train Loss: 1.186 | Val. Loss: 0.766 | Train Pearson: 0.325 | Val. Pearson: 0.291\n",
            "| Epoch: 14 | Train Loss: 1.181 | Val. Loss: 0.766 | Train Pearson: 0.329 | Val. Pearson: 0.293\n",
            "| Epoch: 15 | Train Loss: 1.176 | Val. Loss: 0.765 | Train Pearson: 0.333 | Val. Pearson: 0.296\n",
            "| Epoch: 16 | Train Loss: 1.172 | Val. Loss: 0.764 | Train Pearson: 0.336 | Val. Pearson: 0.297\n",
            "| Epoch: 17 | Train Loss: 1.167 | Val. Loss: 0.764 | Train Pearson: 0.339 | Val. Pearson: 0.299\n",
            "| Epoch: 18 | Train Loss: 1.162 | Val. Loss: 0.764 | Train Pearson: 0.342 | Val. Pearson: 0.301\n",
            "| Epoch: 19 | Train Loss: 1.158 | Val. Loss: 0.763 | Train Pearson: 0.345 | Val. Pearson: 0.303\n",
            "| Epoch: 20 | Train Loss: 1.155 | Val. Loss: 0.762 | Train Pearson: 0.347 | Val. Pearson: 0.305\n",
            "| Epoch: 21 | Train Loss: 1.153 | Val. Loss: 0.762 | Train Pearson: 0.350 | Val. Pearson: 0.306\n",
            "| Epoch: 22 | Train Loss: 1.150 | Val. Loss: 0.762 | Train Pearson: 0.352 | Val. Pearson: 0.308\n",
            "| Epoch: 23 | Train Loss: 1.148 | Val. Loss: 0.761 | Train Pearson: 0.354 | Val. Pearson: 0.309\n",
            "| Epoch: 24 | Train Loss: 1.145 | Val. Loss: 0.761 | Train Pearson: 0.357 | Val. Pearson: 0.310\n",
            "| Epoch: 25 | Train Loss: 1.143 | Val. Loss: 0.761 | Train Pearson: 0.359 | Val. Pearson: 0.311\n",
            "| Epoch: 26 | Train Loss: 1.140 | Val. Loss: 0.760 | Train Pearson: 0.362 | Val. Pearson: 0.312\n",
            "| Epoch: 27 | Train Loss: 1.138 | Val. Loss: 0.760 | Train Pearson: 0.364 | Val. Pearson: 0.313\n",
            "| Epoch: 28 | Train Loss: 1.135 | Val. Loss: 0.760 | Train Pearson: 0.366 | Val. Pearson: 0.314\n",
            "| Epoch: 29 | Train Loss: 1.133 | Val. Loss: 0.760 | Train Pearson: 0.368 | Val. Pearson: 0.315\n",
            "| Epoch: 30 | Train Loss: 1.131 | Val. Loss: 0.759 | Train Pearson: 0.371 | Val. Pearson: 0.315\n",
            "| Epoch: 31 | Train Loss: 1.127 | Val. Loss: 0.758 | Train Pearson: 0.373 | Val. Pearson: 0.316\n",
            "| Epoch: 32 | Train Loss: 1.122 | Val. Loss: 0.758 | Train Pearson: 0.375 | Val. Pearson: 0.317\n",
            "| Epoch: 33 | Train Loss: 1.117 | Val. Loss: 0.758 | Train Pearson: 0.378 | Val. Pearson: 0.318\n",
            "| Epoch: 34 | Train Loss: 1.114 | Val. Loss: 0.758 | Train Pearson: 0.380 | Val. Pearson: 0.319\n",
            "| Epoch: 35 | Train Loss: 1.110 | Val. Loss: 0.758 | Train Pearson: 0.383 | Val. Pearson: 0.320\n",
            "| Epoch: 36 | Train Loss: 1.108 | Val. Loss: 0.757 | Train Pearson: 0.385 | Val. Pearson: 0.320\n",
            "| Epoch: 37 | Train Loss: 1.106 | Val. Loss: 0.757 | Train Pearson: 0.388 | Val. Pearson: 0.321\n",
            "| Epoch: 38 | Train Loss: 1.103 | Val. Loss: 0.757 | Train Pearson: 0.390 | Val. Pearson: 0.322\n",
            "| Epoch: 39 | Train Loss: 1.103 | Val. Loss: 0.756 | Train Pearson: 0.392 | Val. Pearson: 0.323\n",
            "| Epoch: 40 | Train Loss: 1.101 | Val. Loss: 0.756 | Train Pearson: 0.396 | Val. Pearson: 0.324\n",
            "| Epoch: 41 | Train Loss: 1.099 | Val. Loss: 0.755 | Train Pearson: 0.399 | Val. Pearson: 0.325\n",
            "| Epoch: 42 | Train Loss: 1.096 | Val. Loss: 0.755 | Train Pearson: 0.401 | Val. Pearson: 0.325\n",
            "| Epoch: 43 | Train Loss: 1.093 | Val. Loss: 0.755 | Train Pearson: 0.405 | Val. Pearson: 0.326\n",
            "| Epoch: 44 | Train Loss: 1.089 | Val. Loss: 0.754 | Train Pearson: 0.407 | Val. Pearson: 0.327\n",
            "| Epoch: 45 | Train Loss: 1.086 | Val. Loss: 0.754 | Train Pearson: 0.411 | Val. Pearson: 0.328\n",
            "| Epoch: 46 | Train Loss: 1.083 | Val. Loss: 0.754 | Train Pearson: 0.414 | Val. Pearson: 0.328\n",
            "| Epoch: 47 | Train Loss: 1.081 | Val. Loss: 0.754 | Train Pearson: 0.416 | Val. Pearson: 0.328\n",
            "| Epoch: 48 | Train Loss: 1.079 | Val. Loss: 0.754 | Train Pearson: 0.419 | Val. Pearson: 0.329\n",
            "| Epoch: 49 | Train Loss: 1.077 | Val. Loss: 0.754 | Train Pearson: 0.422 | Val. Pearson: 0.329\n",
            "| Epoch: 50 | Train Loss: 1.073 | Val. Loss: 0.755 | Train Pearson: 0.424 | Val. Pearson: 0.330\n",
            "| Epoch: 51 | Train Loss: 1.071 | Val. Loss: 0.753 | Train Pearson: 0.427 | Val. Pearson: 0.330\n",
            "| Epoch: 52 | Train Loss: 1.068 | Val. Loss: 0.755 | Train Pearson: 0.430 | Val. Pearson: 0.330\n",
            "| Epoch: 53 | Train Loss: 1.065 | Val. Loss: 0.756 | Train Pearson: 0.432 | Val. Pearson: 0.330\n",
            "| Epoch: 54 | Train Loss: 1.062 | Val. Loss: 0.754 | Train Pearson: 0.435 | Val. Pearson: 0.331\n",
            "| Epoch: 55 | Train Loss: 1.059 | Val. Loss: 0.755 | Train Pearson: 0.437 | Val. Pearson: 0.330\n",
            "| Epoch: 56 | Train Loss: 1.056 | Val. Loss: 0.754 | Train Pearson: 0.440 | Val. Pearson: 0.331\n",
            "| Epoch: 57 | Train Loss: 1.054 | Val. Loss: 0.756 | Train Pearson: 0.442 | Val. Pearson: 0.330\n",
            "| Epoch: 58 | Train Loss: 1.049 | Val. Loss: 0.758 | Train Pearson: 0.445 | Val. Pearson: 0.329\n",
            "| Epoch: 59 | Train Loss: 1.046 | Val. Loss: 0.757 | Train Pearson: 0.447 | Val. Pearson: 0.330\n",
            "| Epoch: 60 | Train Loss: 1.043 | Val. Loss: 0.757 | Train Pearson: 0.450 | Val. Pearson: 0.330\n",
            "| Epoch: 61 | Train Loss: 1.040 | Val. Loss: 0.758 | Train Pearson: 0.452 | Val. Pearson: 0.330\n",
            "| Epoch: 62 | Train Loss: 1.039 | Val. Loss: 0.756 | Train Pearson: 0.456 | Val. Pearson: 0.331\n",
            "| Epoch: 63 | Train Loss: 1.033 | Val. Loss: 0.758 | Train Pearson: 0.458 | Val. Pearson: 0.330\n",
            "| Epoch: 64 | Train Loss: 1.034 | Val. Loss: 0.757 | Train Pearson: 0.461 | Val. Pearson: 0.331\n",
            "| Epoch: 65 | Train Loss: 1.030 | Val. Loss: 0.759 | Train Pearson: 0.463 | Val. Pearson: 0.330\n",
            "| Epoch: 66 | Train Loss: 1.026 | Val. Loss: 0.758 | Train Pearson: 0.466 | Val. Pearson: 0.330\n",
            "| Epoch: 67 | Train Loss: 1.024 | Val. Loss: 0.759 | Train Pearson: 0.468 | Val. Pearson: 0.330\n",
            "| Epoch: 68 | Train Loss: 1.019 | Val. Loss: 0.760 | Train Pearson: 0.470 | Val. Pearson: 0.329\n",
            "| Epoch: 69 | Train Loss: 1.016 | Val. Loss: 0.760 | Train Pearson: 0.472 | Val. Pearson: 0.329\n",
            "| Epoch: 70 | Train Loss: 1.013 | Val. Loss: 0.761 | Train Pearson: 0.474 | Val. Pearson: 0.329\n",
            "| Epoch: 71 | Train Loss: 1.010 | Val. Loss: 0.761 | Train Pearson: 0.476 | Val. Pearson: 0.329\n",
            "| Epoch: 72 | Train Loss: 1.004 | Val. Loss: 0.762 | Train Pearson: 0.479 | Val. Pearson: 0.328\n",
            "| Epoch: 73 | Train Loss: 1.000 | Val. Loss: 0.763 | Train Pearson: 0.480 | Val. Pearson: 0.328\n",
            "| Epoch: 74 | Train Loss: 1.000 | Val. Loss: 0.761 | Train Pearson: 0.484 | Val. Pearson: 0.328\n",
            "| Epoch: 75 | Train Loss: 0.997 | Val. Loss: 0.762 | Train Pearson: 0.485 | Val. Pearson: 0.327\n",
            "| Epoch: 76 | Train Loss: 0.993 | Val. Loss: 0.763 | Train Pearson: 0.487 | Val. Pearson: 0.328\n",
            "| Epoch: 77 | Train Loss: 0.989 | Val. Loss: 0.764 | Train Pearson: 0.489 | Val. Pearson: 0.328\n",
            "| Epoch: 78 | Train Loss: 0.986 | Val. Loss: 0.763 | Train Pearson: 0.491 | Val. Pearson: 0.328\n",
            "| Epoch: 79 | Train Loss: 0.984 | Val. Loss: 0.765 | Train Pearson: 0.493 | Val. Pearson: 0.327\n",
            "| Epoch: 80 | Train Loss: 0.977 | Val. Loss: 0.765 | Train Pearson: 0.495 | Val. Pearson: 0.326\n",
            "| Epoch: 81 | Train Loss: 0.974 | Val. Loss: 0.767 | Train Pearson: 0.497 | Val. Pearson: 0.326\n",
            "| Epoch: 82 | Train Loss: 0.972 | Val. Loss: 0.766 | Train Pearson: 0.500 | Val. Pearson: 0.325\n",
            "| Epoch: 83 | Train Loss: 0.967 | Val. Loss: 0.768 | Train Pearson: 0.502 | Val. Pearson: 0.325\n",
            "| Epoch: 84 | Train Loss: 0.965 | Val. Loss: 0.769 | Train Pearson: 0.504 | Val. Pearson: 0.325\n",
            "| Epoch: 85 | Train Loss: 0.962 | Val. Loss: 0.768 | Train Pearson: 0.507 | Val. Pearson: 0.324\n",
            "| Epoch: 86 | Train Loss: 0.961 | Val. Loss: 0.770 | Train Pearson: 0.509 | Val. Pearson: 0.323\n",
            "| Epoch: 87 | Train Loss: 0.956 | Val. Loss: 0.769 | Train Pearson: 0.511 | Val. Pearson: 0.323\n",
            "| Epoch: 88 | Train Loss: 0.957 | Val. Loss: 0.773 | Train Pearson: 0.512 | Val. Pearson: 0.322\n",
            "| Epoch: 89 | Train Loss: 0.955 | Val. Loss: 0.773 | Train Pearson: 0.514 | Val. Pearson: 0.322\n",
            "| Epoch: 90 | Train Loss: 0.951 | Val. Loss: 0.773 | Train Pearson: 0.517 | Val. Pearson: 0.322\n",
            "| Epoch: 91 | Train Loss: 0.946 | Val. Loss: 0.773 | Train Pearson: 0.519 | Val. Pearson: 0.320\n",
            "| Epoch: 92 | Train Loss: 0.942 | Val. Loss: 0.776 | Train Pearson: 0.520 | Val. Pearson: 0.320\n",
            "| Epoch: 93 | Train Loss: 0.940 | Val. Loss: 0.777 | Train Pearson: 0.522 | Val. Pearson: 0.321\n",
            "| Epoch: 94 | Train Loss: 0.937 | Val. Loss: 0.777 | Train Pearson: 0.524 | Val. Pearson: 0.320\n",
            "| Epoch: 95 | Train Loss: 0.931 | Val. Loss: 0.778 | Train Pearson: 0.526 | Val. Pearson: 0.321\n",
            "| Epoch: 96 | Train Loss: 0.931 | Val. Loss: 0.778 | Train Pearson: 0.527 | Val. Pearson: 0.321\n",
            "| Epoch: 97 | Train Loss: 0.931 | Val. Loss: 0.782 | Train Pearson: 0.528 | Val. Pearson: 0.319\n",
            "| Epoch: 98 | Train Loss: 0.921 | Val. Loss: 0.780 | Train Pearson: 0.531 | Val. Pearson: 0.319\n",
            "| Epoch: 99 | Train Loss: 0.910 | Val. Loss: 0.780 | Train Pearson: 0.533 | Val. Pearson: 0.319\n",
            "| Epoch: 100 | Train Loss: 0.910 | Val. Loss: 0.782 | Train Pearson: 0.535 | Val. Pearson: 0.318\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a16HCkvKERj",
        "colab_type": "code",
        "outputId": "b6b6dea5-048f-4a67-ac68-9403f293cd97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import scipy\n",
        "\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "  predictions = model(mean_val_t).squeeze(1)\n",
        "  \n",
        "  pearson_corr = scipy.stats.pearsonr(predictions, val_scores_t)\n",
        "\n",
        "  print(predictions)\n",
        "  print(pearson_corr)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([-5.7613e-01, -4.2422e-01, -3.2663e-01, -3.4373e-01, -2.7768e-01,\n",
            "         3.1056e-01, -8.0722e-01,  5.3888e-01,  3.9542e-01,  5.0608e-01,\n",
            "        -5.3578e-01, -4.7029e-01, -5.0387e-01,  3.9389e-01, -3.6459e-01,\n",
            "        -5.0361e-01,  4.5471e-01, -1.0749e-01, -4.4565e-01, -1.8392e-01,\n",
            "        -1.2759e-01, -2.0696e-02, -2.3228e-01, -2.6273e-01, -2.0555e-01,\n",
            "        -5.6314e-01, -1.2995e-01,  1.8809e-01, -6.7057e-02, -4.0258e-01,\n",
            "        -3.2406e-02, -6.8032e-01, -4.4420e-01,  2.1091e-01,  1.2197e-01,\n",
            "         1.8178e-01, -4.2727e-01, -1.6787e-01, -4.0592e-01, -3.8352e-01,\n",
            "        -1.4399e-01, -3.0573e-01, -7.6351e-02,  1.9145e-01, -5.1621e-01,\n",
            "        -3.6085e-01, -8.0244e-01,  8.2469e-02, -1.0620e-01, -3.6718e-01,\n",
            "         3.1615e-01,  9.8524e-02,  2.1264e-01, -3.6903e-01,  8.5483e-02,\n",
            "         9.5246e-02, -1.6452e-01, -7.9122e-01, -6.3585e-01, -7.0090e-02,\n",
            "        -3.8040e-01, -2.7578e-02,  2.8637e-01, -4.2717e-01,  2.2785e-01,\n",
            "        -3.6628e-01,  3.6359e-01,  4.0958e-01, -6.5329e-02, -5.1415e-01,\n",
            "        -5.6397e-02, -6.2988e-01, -5.2376e-01, -3.3186e-01,  1.1318e-01,\n",
            "         2.9610e-01, -3.0657e-01,  2.5259e-01,  3.6508e-02, -2.6858e-02,\n",
            "         5.2770e-02,  3.2191e-02, -2.2328e-01, -4.3726e-01, -2.1913e-01,\n",
            "         2.1523e-01, -3.3252e-01,  1.1580e-01, -3.1322e-03, -7.6592e-01,\n",
            "        -1.9838e-01,  1.6042e-01,  2.7893e-01,  8.1755e-03, -5.8731e-01,\n",
            "        -5.5709e-01, -3.2355e-01, -1.7160e-01,  2.3527e-02, -8.9876e-02,\n",
            "         2.8968e-01, -1.9451e-02, -2.1336e-01, -3.4042e-01, -3.4130e-02,\n",
            "        -1.6331e-01, -5.0534e-01, -5.0955e-01, -4.7542e-01, -1.7337e-01,\n",
            "        -4.7370e-01, -4.2786e-01,  3.3444e-01,  2.5415e-01,  1.7010e-01,\n",
            "        -1.1572e-02, -4.4089e-02, -6.7667e-02, -4.6364e-01,  2.0438e-01,\n",
            "        -5.2054e-01, -3.5624e-01, -2.9895e-01, -3.0028e-01, -2.2070e-01,\n",
            "        -1.4864e-01, -2.6082e-01, -1.4488e-02,  5.2995e-01,  1.8328e-01,\n",
            "         3.8489e-01,  4.6674e-01, -2.2660e-01, -4.3094e-01,  4.1835e-02,\n",
            "         1.4237e-01, -1.5627e-01, -3.9671e-02,  3.6447e-01, -3.0427e-01,\n",
            "         1.0325e-01,  4.4903e-02,  7.6378e-02,  1.0216e-01,  3.2683e-01,\n",
            "         4.4223e-02,  2.3411e-02, -1.9643e-02, -3.0779e-02, -1.0327e-01,\n",
            "        -2.3899e-01,  1.7868e-01, -1.0200e-01, -4.3505e-01,  2.3694e-01,\n",
            "        -4.6175e-01,  1.6132e-01, -1.2581e-01, -2.6010e-01, -3.1971e-01,\n",
            "        -3.2584e-01,  8.5813e-02, -7.5045e-01, -4.0544e-01, -2.9237e-01,\n",
            "        -1.9580e-01, -4.3141e-02, -7.8602e-02,  1.4106e-01,  3.8240e-01,\n",
            "         3.2976e-01,  4.0449e-01, -3.9651e-01,  1.0155e-01, -4.7067e-01,\n",
            "        -1.2392e-01, -1.7419e-01,  4.6842e-01, -6.4276e-01, -3.0987e-01,\n",
            "         3.0589e-01, -3.0491e-01, -6.8055e-02, -4.1022e-01, -3.9862e-01,\n",
            "        -1.8010e-01, -2.4816e-01, -6.4182e-01, -3.8849e-01,  3.3226e-01,\n",
            "        -7.0354e-01,  3.0583e-01, -4.4287e-01,  4.4680e-02, -2.9098e-02,\n",
            "        -3.5371e-01, -3.4072e-01,  2.7184e-01,  2.0239e-01,  5.4481e-02,\n",
            "        -4.0952e-01,  3.5678e-01, -2.2899e-01, -2.6355e-01,  3.4528e-01,\n",
            "         3.7425e-01, -5.9932e-02,  2.7643e-01,  1.7980e-02, -2.2643e-01,\n",
            "        -3.1471e-01,  2.6920e-02, -2.6732e-01, -1.0833e-01, -2.9089e-01,\n",
            "        -2.7718e-01, -2.9367e-01,  1.9235e-01,  8.2807e-02, -2.3803e-01,\n",
            "        -6.9425e-01, -1.5078e-01, -2.2645e-01, -7.1448e-02, -2.9047e-01,\n",
            "        -1.6777e-01,  1.0190e-01,  2.6035e-01, -3.4053e-02, -2.4245e-01,\n",
            "        -6.7923e-02, -1.4759e-01, -6.6809e-01, -4.9091e-01, -9.1737e-02,\n",
            "        -3.6395e-01, -6.0019e-01, -6.9063e-01, -1.5393e-01,  2.9617e-01,\n",
            "        -6.2291e-02,  3.4155e-01, -1.8072e-01,  3.9483e-01, -7.1758e-01,\n",
            "        -2.2910e-01, -3.3761e-01, -2.7116e-01,  7.8178e-02, -1.5782e-01,\n",
            "        -6.2483e-01,  1.7650e-01, -3.1526e-01, -6.0942e-01, -6.0644e-01,\n",
            "         5.1791e-01, -2.4961e-01, -5.5963e-01,  2.5692e-01,  6.7255e-01,\n",
            "        -4.8859e-01, -1.4304e-01,  1.6394e-01, -4.2142e-02,  2.9560e-01,\n",
            "        -8.5262e-03,  4.2643e-01,  5.1453e-01, -7.3995e-01,  1.3627e-01,\n",
            "        -1.6856e-01,  1.6279e-01, -4.3213e-01, -8.3030e-01, -1.5068e-01,\n",
            "        -2.5813e-01, -1.2962e-01, -5.3839e-01, -2.5924e-01,  3.8626e-01,\n",
            "         5.0771e-02, -5.2851e-01, -3.0329e-01, -2.5190e-01, -3.3859e-01,\n",
            "        -3.1076e-01, -4.7240e-01, -2.2833e-01,  8.7972e-02,  1.3174e-01,\n",
            "        -4.9117e-01, -6.9138e-01,  4.2191e-01, -3.0841e-01,  2.3332e-01,\n",
            "         1.9716e-01, -5.2373e-01, -8.6782e-02, -4.1892e-01,  1.1209e-01,\n",
            "        -4.3010e-01, -6.9164e-02, -2.8113e-01,  3.6694e-01, -2.5618e-01,\n",
            "        -7.7655e-01,  1.6329e-01,  3.7695e-01, -3.0615e-01,  5.6744e-05,\n",
            "        -3.3862e-01, -3.4602e-01,  4.0397e-01, -2.5492e-01, -1.2867e-01,\n",
            "        -1.4884e-01, -6.4247e-01,  3.3577e-02,  5.5721e-01,  5.4380e-02,\n",
            "         1.0106e-01, -2.4541e-01, -2.0939e-01, -1.0213e-01,  3.7170e-01,\n",
            "        -6.9289e-01,  2.6223e-01,  1.5820e-01,  2.8897e-01,  4.5494e-02,\n",
            "        -7.1192e-02, -5.4996e-02, -5.6302e-01, -6.1415e-02, -4.8601e-01,\n",
            "        -4.7714e-01,  1.8355e-01, -2.0996e-01, -4.9670e-01, -4.1651e-01,\n",
            "        -3.1884e-01,  3.7203e-01,  3.5195e-01,  2.2700e-01,  6.4146e-01,\n",
            "        -4.4197e-01, -3.3337e-01,  3.5251e-02,  1.8030e-01, -5.4149e-01,\n",
            "        -5.0018e-02,  4.8217e-01, -4.4497e-01,  2.6128e-01,  2.5483e-01,\n",
            "        -1.8686e-01, -3.3328e-01,  3.4107e-01, -2.5204e-01,  1.8948e-01,\n",
            "         2.1648e-01, -1.4310e-01, -4.1168e-01,  3.1459e-01, -1.6893e-01,\n",
            "        -7.3527e-01, -7.2263e-01, -4.4334e-02,  2.1463e-01, -3.1750e-02,\n",
            "        -4.1503e-01, -6.0095e-01,  5.3931e-02, -1.5955e-01, -1.6425e-01,\n",
            "        -1.7144e-01,  2.9000e-01, -7.6894e-01, -5.3791e-02, -5.7992e-01,\n",
            "        -2.5791e-01,  2.6389e-01,  1.1213e-01, -5.4136e-01, -8.5363e-01,\n",
            "        -3.0175e-02, -8.6237e-02,  2.0778e-01, -3.2994e-01, -6.3358e-01,\n",
            "        -5.5874e-01,  2.6174e-01, -5.9531e-01, -2.6225e-02,  3.2160e-01,\n",
            "        -3.9787e-01, -3.3901e-02,  6.1511e-01,  1.1916e-01,  1.4543e-01,\n",
            "        -1.0525e-01, -3.6648e-01,  3.2200e-01, -3.9415e-01, -3.2690e-01,\n",
            "         2.5384e-01,  6.5438e-01, -2.9413e-01, -6.0032e-01,  1.0612e-01,\n",
            "         1.9725e-01,  2.3132e-01,  4.3579e-01,  2.4298e-02, -6.7536e-01,\n",
            "        -5.9287e-02,  3.5528e-01, -3.8536e-01,  1.4323e-01, -2.2647e-01,\n",
            "         1.3040e-01,  5.4002e-01, -3.9608e-01,  1.9082e-02,  2.0244e-01,\n",
            "        -6.7251e-01, -4.3571e-01, -2.6706e-01,  5.8166e-01,  5.5353e-01,\n",
            "        -2.6894e-01,  9.6972e-02, -2.9345e-01, -5.0112e-01,  1.5962e-01,\n",
            "        -2.8643e-01, -7.7626e-01, -3.1375e-01, -2.2700e-01, -4.2608e-01,\n",
            "        -7.7819e-01,  2.6392e-01, -3.5455e-01, -4.3533e-01, -4.9080e-01,\n",
            "        -2.2677e-01, -6.2005e-01,  4.5006e-01,  7.4062e-01, -4.6904e-01,\n",
            "        -7.2591e-02, -1.0840e-01, -1.2530e-01, -1.2203e-01, -2.9559e-01,\n",
            "         3.8275e-02,  4.3870e-01, -5.8234e-01,  2.5983e-01,  1.3476e-01,\n",
            "         4.8331e-01, -5.2094e-01,  1.5104e-01, -2.2984e-01,  2.8053e-01,\n",
            "        -4.1403e-01, -9.4036e-02, -1.5917e-01, -4.5620e-01, -4.9282e-01,\n",
            "        -6.7364e-02,  3.3398e-01,  1.1265e-01, -7.1240e-02, -3.4632e-01,\n",
            "         2.2929e-01, -4.3185e-01,  2.3914e-03, -3.2364e-01,  4.1524e-01,\n",
            "        -4.9952e-01,  2.7150e-01,  8.4037e-02,  3.4079e-01, -8.5483e-01,\n",
            "        -6.2498e-01, -1.4043e-01, -1.1082e-01, -5.2098e-01, -1.0047e-01,\n",
            "         5.4216e-02,  4.2946e-01,  1.4472e-01, -5.6441e-01, -2.6299e-01,\n",
            "        -7.5953e-02,  2.4295e-01,  3.8820e-01, -2.9668e-01,  4.5872e-02,\n",
            "        -5.1926e-01, -8.3899e-04,  2.9560e-01, -5.5083e-01,  1.8845e-01,\n",
            "         6.0631e-01,  3.4135e-01, -1.9347e-01,  2.9460e-01, -1.0054e-02,\n",
            "        -5.3882e-02, -3.6466e-01,  1.2674e-01, -4.8612e-01, -6.1295e-01,\n",
            "        -4.8454e-01, -4.5401e-01, -3.3804e-01, -3.4318e-01,  5.5198e-02,\n",
            "        -8.9065e-02, -3.4820e-02, -5.0680e-01,  6.1392e-01,  2.6193e-01,\n",
            "        -7.4618e-02, -1.2835e-01, -3.2955e-01, -4.1663e-02, -2.5609e-01,\n",
            "        -3.3895e-01, -4.1717e-01, -3.7401e-01, -9.3998e-02,  2.9489e-01,\n",
            "        -4.8451e-01, -4.8168e-01,  4.3210e-01,  7.4846e-02, -4.9792e-02,\n",
            "        -2.9583e-01, -1.1163e-01, -6.4574e-02, -2.8124e-01, -1.8973e-01,\n",
            "         7.3316e-02, -4.7812e-01, -7.9078e-02, -2.8897e-01, -3.1167e-01,\n",
            "         1.1815e-01, -4.2079e-01, -2.7091e-01,  9.4496e-02, -7.2598e-01,\n",
            "        -6.2966e-02, -9.7158e-02, -4.1385e-01, -2.4081e-02, -2.3274e-01,\n",
            "         1.5029e-01, -1.3313e-01, -1.2105e-01, -6.8313e-01,  1.8020e-01,\n",
            "         2.2489e-01,  2.0741e-01, -3.9329e-01, -2.9137e-02, -2.3117e-01,\n",
            "        -1.4476e-01,  6.3622e-01,  2.3367e-01,  2.7296e-01, -4.2253e-01,\n",
            "        -1.3494e-02, -6.2600e-02, -1.8190e-01,  2.2701e-02, -2.8052e-01,\n",
            "         3.8092e-01, -9.9879e-02, -6.6659e-01, -4.6758e-02, -3.1109e-01,\n",
            "         4.3348e-01, -2.8080e-01,  4.1432e-01,  1.8582e-01, -7.3937e-01,\n",
            "         2.4104e-01, -4.4727e-01,  3.7178e-01,  4.4929e-01,  5.9371e-01,\n",
            "        -1.2528e-02,  1.0372e-01,  1.3487e-01, -7.1701e-01,  8.2983e-03,\n",
            "         3.8140e-01, -4.7201e-02,  1.5305e-01, -2.9029e-01, -2.3868e-01,\n",
            "        -6.0489e-01,  4.1198e-01,  2.0597e-02,  2.6128e-01,  3.8430e-01,\n",
            "         2.0708e-01,  7.2832e-02, -7.6291e-01,  2.3665e-02,  3.7484e-01,\n",
            "         2.0629e-01,  3.3793e-01, -5.2690e-01, -4.5464e-01,  5.7511e-01,\n",
            "        -5.4108e-01, -2.5642e-01,  2.7309e-01, -3.5544e-01, -4.9438e-02,\n",
            "        -1.1073e-01, -5.6574e-01,  8.2775e-03, -5.1166e-01, -5.7239e-01,\n",
            "        -1.6163e-01, -3.3965e-01,  2.3639e-01, -2.3230e-01, -5.6755e-01,\n",
            "        -2.3466e-01,  3.2913e-01,  3.8182e-01,  6.8501e-02,  2.7244e-01,\n",
            "        -6.8331e-01,  1.3633e-01, -5.2880e-01,  2.5282e-01,  2.9965e-01,\n",
            "         4.4508e-02, -3.5889e-01,  7.5636e-01,  2.7230e-01,  1.2956e-01,\n",
            "         2.7032e-01,  2.7515e-01,  1.9818e-01, -2.3397e-01,  1.0091e-02,\n",
            "         1.6832e-01, -4.1180e-01, -6.7819e-01, -2.9701e-02,  1.1787e-01,\n",
            "         1.4056e-02, -1.0019e-02,  2.6613e-01, -5.8867e-02, -3.4205e-01,\n",
            "        -1.5288e-01, -2.4403e-01,  9.9824e-03,  8.2769e-01, -3.6255e-01,\n",
            "        -8.3093e-01, -2.5967e-01, -5.2648e-01, -4.8464e-01,  1.9273e-01,\n",
            "        -6.5483e-01, -5.6077e-01, -7.3072e-04,  5.3673e-02, -2.5101e-01,\n",
            "        -5.8589e-01,  1.2740e-01, -1.9503e-01, -1.3819e-01, -1.6300e-01,\n",
            "         2.8894e-01, -4.1218e-01,  2.2137e-01, -2.4445e-02, -4.5177e-01,\n",
            "         3.1591e-01, -2.7774e-01, -2.3855e-01,  2.7134e-01, -3.6985e-01,\n",
            "         1.2399e-01, -5.5473e-01,  6.5943e-02,  4.1987e-01,  5.0753e-02,\n",
            "        -2.5409e-01, -1.4097e-01, -2.1253e-01,  1.6374e-02, -1.4651e-03,\n",
            "        -2.8172e-01,  3.9525e-01,  2.3433e-01, -3.6632e-01, -4.7331e-02,\n",
            "         1.8927e-01, -1.9832e-01,  1.8461e-01,  2.2863e-02, -5.7950e-01,\n",
            "        -5.6934e-01,  5.5970e-02, -3.4547e-01,  1.0960e-01, -2.6451e-01,\n",
            "        -4.1695e-01, -7.1595e-01, -2.1557e-01, -5.3320e-01,  1.7002e-01,\n",
            "        -6.6082e-02, -3.1572e-01, -9.2911e-02,  1.7889e-01, -1.1702e-01,\n",
            "         6.2687e-01,  2.1755e-01, -3.1304e-01, -2.4035e-01, -7.2910e-01,\n",
            "         2.9387e-01, -1.6551e-02, -4.4560e-01,  1.6158e-01, -4.3513e-01,\n",
            "        -6.4493e-01,  7.5108e-01, -5.8679e-02, -3.2303e-01, -6.5502e-01,\n",
            "         2.3991e-01,  2.1755e-01, -2.6430e-01, -3.7593e-01, -1.5044e-02,\n",
            "        -1.2779e-01, -4.4356e-02, -5.2850e-02,  1.5463e-01, -3.5771e-01,\n",
            "        -2.9187e-01, -2.4907e-01,  1.9959e-01,  3.0036e-01, -7.7197e-03,\n",
            "         1.2504e-01, -5.4869e-01, -2.2949e-01,  4.5293e-01,  1.1990e-01,\n",
            "         4.3252e-01,  1.2494e-01, -5.3155e-01, -2.7055e-01, -6.1097e-01,\n",
            "         3.2869e-01, -4.6150e-01,  1.4890e-01,  1.7676e-01,  2.4741e-01,\n",
            "        -6.5548e-01,  7.0514e-02, -4.3107e-01,  1.7214e-02, -1.3551e-01,\n",
            "         3.3643e-01, -2.0123e-01,  1.9436e-01, -4.4978e-01, -3.1505e-01,\n",
            "        -9.7671e-02,  5.0087e-02, -2.2041e-01,  5.3504e-01,  2.5271e-01,\n",
            "        -9.5572e-02, -1.2307e-03,  2.0121e-01, -7.9568e-02, -6.8677e-02,\n",
            "        -6.7080e-02, -3.2531e-01, -3.0547e-01, -6.8496e-01, -8.6184e-01,\n",
            "        -4.3885e-01,  1.9096e-01, -2.1200e-02, -7.6418e-02,  4.2367e-01,\n",
            "        -2.2963e-01, -5.1251e-02, -1.9596e-01, -4.2261e-01,  1.8307e-01,\n",
            "         4.4902e-01, -6.6385e-02,  3.3632e-01, -4.0960e-01, -5.3466e-01,\n",
            "         1.4143e-01, -7.4710e-01,  1.0801e-01, -4.6820e-01,  5.1169e-02,\n",
            "        -1.5344e-01, -2.5245e-01, -2.2050e-01, -8.1639e-02, -2.4408e-01,\n",
            "        -7.2734e-02,  1.3484e-01, -4.5031e-01,  2.4982e-02,  2.1983e-01,\n",
            "        -6.9459e-01, -1.3428e-01, -4.9213e-01,  8.4088e-02, -5.1363e-02,\n",
            "         1.8004e-01, -4.3040e-01, -1.0986e-01, -3.1395e-01,  2.8230e-01,\n",
            "        -2.1747e-01, -2.3652e-01, -6.1618e-01, -3.4535e-01, -1.7949e-01,\n",
            "         5.1272e-01, -2.3513e-01, -5.2520e-01, -2.9759e-01,  8.5685e-02,\n",
            "         2.6243e-01, -2.9704e-01,  1.3542e-03, -5.5307e-01, -1.6493e-01,\n",
            "        -1.8433e-01,  1.4247e-01, -4.3564e-03, -1.8802e-01,  1.3900e-01,\n",
            "        -1.9219e-01, -4.1588e-01, -1.3548e-01, -5.5735e-01,  2.6512e-01,\n",
            "        -4.3539e-01, -7.7808e-02, -4.5276e-01, -4.8561e-01, -1.2603e-01,\n",
            "         1.8311e-01, -4.2292e-01,  1.0144e-01, -9.1229e-02,  1.3831e-01,\n",
            "         5.5211e-01,  3.6110e-02,  5.4429e-03, -4.0116e-01,  4.5014e-01,\n",
            "        -2.3775e-01,  5.5398e-01, -2.9324e-01,  3.0899e-01, -3.8213e-01,\n",
            "         2.3852e-01,  8.0445e-02,  5.0981e-02, -4.0910e-01,  2.1215e-02,\n",
            "        -2.7431e-01, -3.7199e-01, -5.1836e-02, -5.8825e-01,  3.3263e-01,\n",
            "        -1.1967e-01, -6.8934e-01,  1.9622e-01, -4.5841e-01,  7.5105e-03,\n",
            "         2.6598e-02, -5.6677e-01,  2.1458e-01,  6.4847e-01, -1.5408e-01,\n",
            "        -3.4208e-02, -5.5679e-01, -9.2865e-02, -5.9904e-01, -3.6726e-01,\n",
            "        -8.1788e-02,  3.1170e-01, -1.6921e-01, -3.8331e-01, -2.4797e-01,\n",
            "        -5.8492e-01,  2.0772e-01, -5.1236e-03, -4.4877e-01, -6.5172e-01,\n",
            "         1.7860e-01, -5.1946e-01, -1.6787e-01, -2.4210e-02,  4.8728e-01,\n",
            "         1.8473e-01, -2.9877e-01, -2.4085e-01, -1.5461e-01, -2.3042e-01,\n",
            "        -4.7644e-01, -2.5078e-01, -2.4662e-01, -4.1891e-01, -2.2343e-01,\n",
            "         6.2270e-01,  1.1367e-01, -5.1433e-01,  1.1599e-01,  8.4402e-02,\n",
            "         4.5537e-02, -1.4838e-01, -2.8524e-01, -2.2240e-01, -5.8172e-02,\n",
            "         8.9435e-02, -2.7318e-02,  9.0558e-02,  4.7652e-01, -4.4575e-01,\n",
            "         4.0910e-01, -4.3957e-01, -3.4721e-01,  2.7472e-02,  1.2896e-01,\n",
            "        -6.2111e-01, -1.1167e-01,  1.7917e-01, -3.1890e-01, -2.9210e-01,\n",
            "         4.1711e-01, -2.4442e-01, -3.3479e-01, -8.3755e-03, -3.9898e-01,\n",
            "        -5.6815e-01, -6.2539e-01, -2.6916e-01, -3.4032e-01,  3.7035e-02,\n",
            "         8.6702e-03, -1.3822e-01, -4.6004e-01, -4.9142e-01, -3.1055e-01,\n",
            "         6.0164e-01, -1.7262e-01, -4.5474e-01,  7.3901e-02, -4.9275e-01,\n",
            "        -2.5882e-01, -3.7913e-01, -4.2705e-01, -3.8669e-01, -2.1650e-01,\n",
            "        -3.6982e-03, -5.9635e-01,  1.9990e-01, -4.1980e-01,  1.1809e-01,\n",
            "        -2.8566e-01, -2.5527e-01, -3.8262e-01,  7.4595e-02, -2.7111e-01,\n",
            "         9.5830e-02, -2.6890e-01,  5.8715e-01, -5.7880e-01, -3.1766e-01])\n",
            "(0.2915866473296661, 4.749809422612421e-21)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gF1aVLTQJQqr",
        "colab_type": "text"
      },
      "source": [
        "## Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USalvKtRAvQv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from scipy.stats.stats import pearsonr\n",
        "\n",
        "\n",
        "def rmse(predictions, targets):\n",
        "    return np.sqrt(((predictions - targets) ** 2).mean())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bf_aJK0QK8jx",
        "colab_type": "code",
        "outputId": "b947825e-5a5b-41bf-a2b8-3e40eb015400",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "for k in ['linear','poly','rbf','sigmoid']:\n",
        "    clf_t = SVR(kernel=k)\n",
        "    clf_t.fit(mean_train_t, train_scores_t)\n",
        "    print(k)\n",
        "    predictions = torch.tensor(clf_t.predict(mean_val_t))\n",
        "    pearson = pearsonr(val_scores_t, predictions)\n",
        "    mae = mean_absolute_error(val_scores_t, predictions)\n",
        "    print(f'RMSE: {rmse(predictions, val_scores_t)} Pearson {pearson[0]}, MAE {mae}')\n",
        "    print()\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "linear\n",
            "RMSE: 0.9104197100505785 Pearson 0.28177236206307227, MAE 0.6657526608822311\n",
            "\n",
            "poly\n",
            "RMSE: 0.9028564482657653 Pearson 0.30126486089786997, MAE 0.6569537994633171\n",
            "\n",
            "rbf\n",
            "RMSE: 0.8973870196668569 Pearson 0.3316393762543504, MAE 0.6494591305143613\n",
            "\n",
            "sigmoid\n",
            "RMSE: 7.815960344582831 Pearson 0.023161468380610863, MAE 4.750423923801584\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqiEvyWT4533",
        "colab_type": "code",
        "outputId": "8e82cfca-09a2-40f5-9f4d-460d81ed2668",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "clf_t = SVR(kernel='rbf')\n",
        "clf_t.fit(mean_train_val_t, train_val_scores_t)\n",
        "\n",
        "predictions = clf_t.predict(mean_test_t)\n",
        "print(predictions)\n",
        "\n",
        "writeScores(predictions, 'SVR_rbf.txt')\n",
        "files.download('SVR_rbf.txt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 2.72158199e-01  1.22621336e-01  6.68095998e-01  3.27555127e-01\n",
            " -6.72099777e-02 -3.55866291e-02  2.75672045e-04  7.42447047e-01\n",
            "  1.95610696e-01  2.98002333e-01  1.05141316e+00 -1.66610520e-01\n",
            "  1.24349182e-01 -6.17666255e-02  1.22057635e-02  2.56122084e-01\n",
            " -5.87100306e-02 -3.72150821e-02 -2.83545694e-03  5.41129318e-01\n",
            "  3.84357799e-02  4.69772411e-01  4.29898773e-01  9.03572528e-01\n",
            "  1.05517682e-01  3.41356069e-02  3.67741669e-01  4.53923685e-01\n",
            "  1.89974599e-01  1.13849746e-01  2.32724313e-02  2.35024696e-01\n",
            "  1.53019173e-02  2.53089639e-01  3.58803260e-01 -1.46519124e-03\n",
            " -3.18689888e-02  1.30915163e-01  5.01894074e-01  5.29350147e-01\n",
            "  5.52228013e-01  3.58913565e-01  3.07463223e-01  1.02418140e-01\n",
            "  2.62418447e-01  5.23656582e-01 -2.42017784e-01  1.05273707e-01\n",
            " -8.14570844e-02  2.17838297e-01  3.80969380e-01  4.39290413e-01\n",
            "  1.14859235e-01 -5.92210921e-02  4.51709435e-02  1.73901061e-01\n",
            " -3.38875271e-01  6.40323937e-01  2.81913238e-01  6.28312159e-01\n",
            "  1.68475373e-01  5.50289101e-01  2.71657583e-01  5.09995668e-01\n",
            "  3.85428631e-01  6.12188166e-01  2.88353584e-01  4.28325332e-02\n",
            " -1.08724661e-01  5.18036419e-01  4.08436265e-02  4.28777179e-01\n",
            "  2.86990216e-01  3.99178634e-01 -1.54510869e-01  1.13634436e-01\n",
            "  6.83650340e-02 -7.78349351e-02  2.18235482e-01 -7.33790873e-02\n",
            "  1.63745439e-01 -1.80505343e-02 -1.37107673e-01 -9.43483716e-02\n",
            "  3.94393874e-01  3.44113687e-01  5.03064484e-01 -1.35371625e-01\n",
            "  4.15651216e-01  8.53898578e-01  5.91554712e-01  1.00197454e-02\n",
            "  3.62756862e-01  2.72773017e-01  5.41925886e-02  3.70623143e-01\n",
            "  4.26699367e-01  4.12414679e-01 -4.14114918e-01  1.21393626e-01\n",
            " -2.08560785e-02  2.36714645e-01  7.83603733e-01  1.94822313e-01\n",
            "  1.37826205e-01  1.38047735e-01  3.16125461e-01  1.24618880e-01\n",
            " -2.52918972e-02  2.48298921e-01  2.16638245e-01  3.56952291e-01\n",
            "  4.66505463e-01  1.10874120e-01  4.36018206e-01  3.28145002e-01\n",
            "  1.06415711e+00  9.90577869e-02  2.91887080e-01 -3.24442553e-01\n",
            "  3.88125172e-01  3.30842664e-01  7.65041811e-03 -2.58066329e-01\n",
            "  3.01168105e-01  3.43910339e-01  4.73732838e-01 -3.94773019e-01\n",
            "  6.21561743e-01  3.03112472e-01  2.78944533e-01 -1.64034251e-01\n",
            "  1.00976705e-01 -1.62755549e-01  4.81357795e-01  4.43032703e-01\n",
            "  1.60193391e-01  1.02108465e-01  4.59933220e-01  2.43104022e-01\n",
            " -1.63079901e-01  2.43059621e-01  2.11197482e-01  2.34704897e-01\n",
            "  2.30844323e-01  6.60223469e-01  2.87255259e-01  1.05545133e-01\n",
            " -1.88567018e-01  1.28334752e-01 -2.34808367e-01 -4.14583281e-02\n",
            "  9.35332622e-02  4.88024891e-02 -8.75049966e-01 -1.09148334e-01\n",
            "  4.51510990e-01  5.69970506e-01  4.09961967e-01  6.32311029e-01\n",
            "  3.35993543e-01  3.50612958e-01 -9.91697136e-02  2.66258154e-01\n",
            "  3.97951148e-01  1.12561845e-01  3.63709489e-01  6.34502738e-01\n",
            "  5.59436088e-01  2.60155849e-01  4.45707430e-01  4.66931434e-01\n",
            "  3.91074112e-01  5.90814995e-01  3.10818084e-01  4.42311606e-01\n",
            "  2.59674211e-01 -5.80988284e-02 -6.24869722e-02  3.41367938e-01\n",
            "  3.88599339e-03  3.20989080e-01  6.44260633e-01 -4.67612396e-01\n",
            "  2.88107084e-01 -3.10041532e-01  3.68250583e-01 -5.97994481e-02\n",
            "  7.39659303e-01  2.93485759e-01 -2.26095556e-01  7.64226794e-02\n",
            "  3.17865866e-01  5.72594961e-02 -5.20341050e-02  2.32859139e-01\n",
            "  6.32527743e-01  2.29248230e-01  4.37121952e-01  6.11647684e-01\n",
            "  4.13290811e-02  6.54034956e-01  5.50673830e-01  5.80006476e-01\n",
            " -2.53494022e-02  4.19644636e-01  2.07235822e-01  3.88448437e-01\n",
            "  1.17963555e-02  2.13480215e-01 -1.62462805e-01  2.15927908e-01\n",
            "  4.34937444e-04  1.22499895e-01 -5.73547565e-02  3.84799612e-01\n",
            "  1.48798001e-01 -4.85069575e-01  1.67159285e-02  4.04581700e-01\n",
            "  2.62008076e-01  1.63587309e-01  6.94695651e-02 -4.87522060e-02\n",
            "  1.18003946e-01  3.03934737e-01  1.13957722e-01  1.89003292e-01\n",
            "  1.00708185e+00 -4.32870813e-01  4.31895949e-01  2.52077021e-01\n",
            "  3.42527933e-01  2.44866852e-01  4.96182345e-01  6.50829752e-02\n",
            "  1.93495238e-01  4.44457024e-01  4.30742174e-01  4.26577752e-01\n",
            "  3.60889102e-01  8.70621709e-01  4.59557471e-01  3.06194770e-01\n",
            "  8.40180543e-02  5.73653040e-01  7.08372107e-02  3.96468333e-01\n",
            " -2.81584454e-02 -2.56274340e-04 -1.86484682e-01  3.71107737e-01\n",
            "  5.17371529e-01  8.92821755e-02  2.25287379e-01  1.92876589e-01\n",
            "  3.61566616e-01  1.07409215e-01  1.82444391e-01 -3.47921353e-02\n",
            "  1.36717277e-01  3.68079742e-01  1.38418280e-01  3.60851862e-01\n",
            "  8.25317066e-01  3.19862816e-01 -8.40798018e-02 -1.46281454e-01\n",
            "  3.33830136e-02  3.43971831e-01  7.44602238e-01 -1.66438960e-02\n",
            "  5.61311824e-01 -2.68686683e-01  5.84817525e-01  2.55115434e-01\n",
            "  3.95333363e-01  7.35552966e-01  4.87649803e-01  2.19454113e-01\n",
            "  6.37900189e-01 -1.05803311e-01  6.91709785e-01  3.13639170e-01\n",
            "  3.25850442e-01 -5.83760389e-02  2.34602722e-01  3.43769009e-01\n",
            "  9.80344732e-02  6.33633623e-01  3.77791807e-01 -1.84568002e-01\n",
            "  5.87768958e-01  4.23915785e-01  6.96193722e-01  1.39583940e-01\n",
            " -8.04172004e-02 -2.75998296e-02  4.10352761e-01  5.67212388e-02\n",
            "  1.51669197e-01  2.13501435e-01  4.92510746e-02  3.41625957e-01\n",
            "  9.30063689e-01  5.08289554e-01  5.28424145e-01  5.32291462e-01\n",
            "  2.52349263e-01 -1.40645888e-01  3.17042518e-01 -7.86087523e-02\n",
            "  5.20040508e-01 -3.47972789e-01  2.65003386e-01 -1.64326257e-01\n",
            "  2.16068799e-01  4.92203791e-01  5.77554696e-01  5.20640331e-01\n",
            "  4.68209987e-01  1.10105843e-01  4.44434582e-01  5.24922107e-01\n",
            "  7.22964485e-01  4.91195026e-01  3.81823078e-01  1.09405237e-01\n",
            "  2.17482025e-01  5.78489191e-01  5.76337994e-02 -2.16281656e-01\n",
            "  3.13154344e-01 -3.35182622e-02  2.55182580e-01 -1.28690703e-01\n",
            "  4.21306726e-02  2.91355077e-01  1.73371309e-01 -4.37743163e-01\n",
            "  1.29549565e-01  3.29402925e-01  1.66586465e-01  4.44327765e-01\n",
            "  3.90228358e-01 -2.87206681e-01  2.30996588e-01  2.66778394e-01\n",
            "  2.37320850e-01  3.39697641e-01  1.98516450e-01  6.24530044e-01\n",
            "  3.21455874e-01 -1.04352176e-01  1.87017769e-01  1.98263066e-01\n",
            "  3.76141840e-01  3.55386463e-01  6.15231665e-01  2.43260885e-01\n",
            "  6.57981067e-01  1.82431665e-01  1.63849852e-01  1.75798870e-01\n",
            "  2.03511215e-01  7.09391237e-01 -3.72186726e-01 -2.79268254e-01\n",
            "  7.34179160e-01 -8.23954688e-02  1.24341956e-01  4.96187721e-01\n",
            "  4.17617814e-02  4.03730170e-01  3.15916721e-01  8.52690458e-02\n",
            " -4.10721024e-02  7.99572164e-02  3.15992038e-01  1.62335300e-01\n",
            " -7.22445914e-02  9.16341049e-01  7.23330544e-01  3.82243487e-01\n",
            "  1.28357179e-01  3.57312476e-02  9.14627003e-02  6.41124966e-01\n",
            " -5.39710807e-02  6.47153665e-01 -2.26991750e-01  4.56841984e-01\n",
            "  2.66532284e-01  9.94321732e-02  7.24475077e-01  5.68649095e-01\n",
            " -3.77812080e-03  2.77711835e-01  3.77504783e-01 -3.29183190e-01\n",
            "  3.19028397e-01  4.80333580e-01  3.16683731e-01  3.27585687e-01\n",
            "  7.48298256e-01 -1.30032594e-01  7.85652251e-01  1.26340276e-01\n",
            "  4.82206687e-02 -2.77876951e-01  6.85444741e-01  1.31925690e-01\n",
            "  1.62031290e-01 -4.46508983e-02  2.11439241e-01  1.07369328e-01\n",
            "  6.95632318e-01  3.49960516e-01 -3.85399295e-01  2.06047274e-01\n",
            "  1.18696742e-01  2.42535140e-01  4.36955946e-01  3.70011339e-01\n",
            "  2.59219621e-01  1.15851393e-01 -3.54934858e-01  6.06487106e-01\n",
            " -2.12102195e-01  3.45890851e-01 -1.69035547e-01  8.58285119e-02\n",
            "  3.93668588e-02 -3.41586433e-02  2.15476728e-01  1.45860760e-01\n",
            "  4.20632803e-01 -3.37186812e-01  7.52996774e-02  3.91220019e-01\n",
            "  2.14696178e-02  4.50339125e-01 -2.78993852e-02  2.23991867e-01\n",
            "  2.42955084e-01  1.41528941e-01  5.72894635e-01  2.70401370e-01\n",
            "  1.92911701e-01 -1.06782326e-01  2.69271068e-01 -3.09376434e-02\n",
            "  3.59112599e-01 -2.50434923e-01  4.38703499e-02  8.63434141e-02\n",
            "  2.17723682e-01  5.17365244e-01  1.87513739e-01  1.60690898e-01\n",
            "  3.41694124e-01  3.19011513e-01  9.43503646e-01  7.64785948e-01\n",
            "  3.71186074e-01  3.68585195e-01  2.49356660e-01  2.23233031e-01\n",
            "  7.43006903e-01  3.94220338e-01  3.81182829e-01  1.88640732e-01\n",
            "  1.35030447e-01  1.62279660e-01  5.46123975e-01 -4.85997864e-03\n",
            "  3.56261652e-01  1.89791844e-01  4.78882906e-01  2.41184342e-01\n",
            "  2.66083498e-01  6.48855057e-01  1.71255915e-01  4.09020195e-01\n",
            "  1.90629227e-02  3.53572968e-01  1.04568599e+00  2.82446697e-01\n",
            " -2.83384747e-02 -2.16386453e-01  2.35359011e-01  3.84974993e-01\n",
            "  5.57431643e-01  4.17033578e-01  2.04133182e-01  2.91689332e-01\n",
            "  9.51250686e-02  6.39578574e-01  2.64291996e-01 -2.62606778e-01\n",
            "  2.56612013e-01  2.87615861e-01  2.63364502e-01  2.01658723e-01\n",
            "  3.07627176e-01  3.65473018e-01  6.26935711e-01  2.80572827e-01\n",
            "  7.64652430e-02 -4.92507531e-01  3.07921465e-01 -3.31811133e-02\n",
            "  5.19630475e-01  3.99625262e-01  3.13636772e-01  2.48726521e-02\n",
            "  1.05326421e-01 -7.76558725e-03  4.18291813e-01  3.84349862e-01\n",
            "  4.97825525e-01  3.88966797e-01  6.23968818e-02  6.37210774e-01\n",
            "  2.93510965e-01  6.76206230e-02  2.02033503e-01  6.63594202e-02\n",
            "  1.56434684e-01  4.31642289e-01  6.05199217e-01  3.34210030e-01\n",
            "  4.64198350e-01 -1.38858786e-01  1.57684538e-01  5.79966047e-01\n",
            "  4.07822725e-01  7.87701917e-02  3.64993339e-03  4.20175049e-01\n",
            "  1.88798774e-01  4.57415027e-01  1.69258958e-01  6.31947756e-02\n",
            "  5.40138328e-01  2.92095520e-02  2.48015054e-01 -1.76001915e-01\n",
            " -1.33163057e-02  1.41230256e-01  5.17314098e-01 -2.74509728e-01\n",
            "  7.12166816e-03  1.53978906e-01  4.13514519e-03  5.02619122e-02\n",
            "  8.51850577e-01  5.05173188e-01  5.30178060e-01  4.20173938e-01\n",
            "  2.43272875e-01  7.45630046e-02  7.73175358e-01  5.78220115e-01\n",
            "  4.66203169e-01 -6.23395506e-02  4.06417714e-01  3.76258214e-01\n",
            "  4.43878863e-01  2.08883854e-01  1.78977583e-02  4.84346485e-01\n",
            " -1.29595160e-01  8.71839557e-01  2.11968551e-01  4.82483612e-01\n",
            "  7.55432526e-01 -1.18055238e-02  3.03693165e-01 -4.07947698e-02\n",
            "  6.53490784e-01  1.42357680e-01  1.99276803e-01 -1.10125420e-01\n",
            "  1.02915808e-01  1.32415772e-01  4.07072985e-01  2.49157256e-01\n",
            "  9.00654421e-01  2.27624292e-01  2.98094306e-01 -2.87324840e-01\n",
            " -2.02394164e-01  5.92347648e-02 -3.86599207e-02  6.34837123e-01\n",
            " -2.89499430e-01  2.40464650e-01  1.82788537e-01  5.55368636e-01\n",
            "  7.05306504e-02  5.33613454e-01  4.12631789e-01  3.78906118e-02\n",
            " -1.65669127e-01  2.86744394e-01  5.11503142e-01  5.64184902e-01\n",
            "  6.29128954e-01  4.43158652e-01 -6.08215650e-02  1.13083634e-01\n",
            "  2.58889602e-01  4.53119369e-01  3.00523961e-01 -1.35905975e-01\n",
            " -1.76111001e-01  3.00282291e-01  2.70113614e-01  1.00184746e-01\n",
            "  1.17293488e-01  2.23241710e-01  2.99278325e-01  4.89571000e-01\n",
            "  3.82222204e-01  4.12659341e-01 -4.86434245e-02  7.92577736e-02\n",
            "  1.37547240e-01  5.11358134e-01 -2.13750946e-01  1.36042342e-01\n",
            "  1.15572419e-01 -2.75021601e-01  5.40218903e-01  3.49065909e-01\n",
            "  3.13928807e-01  1.34482989e-01  6.75155401e-01  3.15192856e-01\n",
            "  2.29938270e-01  4.48295639e-01  3.08968527e-01  3.79373206e-01\n",
            "  1.26226728e-01 -3.39385563e-02  3.47149989e-01  1.28727019e-01\n",
            " -3.33808967e-01  3.57847104e-01  9.88928845e-01  7.63294446e-02\n",
            " -2.42785717e-01  7.28458757e-02  3.92624991e-02  6.57528456e-02\n",
            "  6.78417177e-02  2.98997095e-01  3.49488830e-01  4.24471511e-01\n",
            " -6.86926810e-02  3.42169144e-01  3.25994570e-02  1.32975923e-01\n",
            "  2.18173598e-01  9.05510196e-01  6.29991979e-01  1.68665077e-01\n",
            "  1.52911047e-01  3.50687201e-02  1.90929682e-01  1.76246847e-01\n",
            "  6.07997794e-01  7.87702062e-01  6.38185497e-01  3.12749845e-01\n",
            "  2.73218590e-01  3.14157456e-01  4.78860074e-01  1.45369249e-01\n",
            "  1.63952049e-02  4.74975655e-01  3.64473690e-01  6.56267076e-01\n",
            "  5.25858823e-01  3.54680225e-02  1.84995107e-01  1.54337393e-01\n",
            "  1.40555188e-01  2.36921087e-01  6.78269119e-02  2.43925775e-01\n",
            " -1.07841842e-01  2.98400208e-01  4.29016223e-01 -1.33669134e-01\n",
            "  5.58116063e-01  5.16230774e-01  5.01368632e-01  5.79069648e-01\n",
            "  4.04587312e-01 -1.72153456e-02  6.00935422e-01  1.01024004e+00\n",
            " -1.27511842e-01  2.57096060e-01  3.12197926e-01  3.91756569e-01\n",
            "  1.84569763e-01  5.80678783e-01 -2.79836966e-01 -1.97562520e-01\n",
            "  2.82813309e-01 -1.22406252e-02  4.24960649e-01 -7.85732970e-02\n",
            "  6.48349654e-01  7.34479152e-01  1.54497606e-01 -8.86357473e-02\n",
            " -1.94752550e-01 -1.56911213e-01 -1.71641158e-01  5.05915867e-01\n",
            "  4.94624162e-01  2.34476666e-01 -2.68925484e-01 -3.36306311e-01\n",
            "  2.39040253e-01  3.25533626e-01  2.57021108e-01  3.96172330e-01\n",
            "  4.39140672e-01  4.12079696e-01  3.77994916e-01  2.80064265e-01\n",
            "  4.86865901e-02  5.24014058e-01 -6.65886242e-01  5.44349113e-01\n",
            "  4.36468321e-01  4.17345191e-01 -7.63756734e-02 -2.54384237e-02\n",
            "  1.66535586e-01 -9.77493954e-02 -1.17890493e-01  9.06877157e-01\n",
            "  8.26827795e-02  1.41831927e-01 -2.88963358e-01  5.00226009e-01\n",
            " -2.37012561e-02  4.54318911e-01  4.76846603e-01  5.83631354e-02\n",
            "  1.95798455e-01  3.31128890e-01  3.96519555e-01  3.31375654e-01\n",
            "  6.07261187e-01 -1.53983448e-01  5.56352149e-01  6.80844732e-01\n",
            "  7.19656944e-02  3.15482018e-01  3.84636924e-01  3.06129908e-01\n",
            "  2.50387290e-01  7.08237909e-02  3.81251239e-01  6.21934336e-01\n",
            "  2.27694697e-01  7.53956478e-02  3.71779595e-02  4.67697750e-01\n",
            "  6.88151148e-01  1.59735603e-01  5.19650287e-01  4.27146367e-01\n",
            "  4.05219675e-01  3.25781121e-01  3.48834008e-01  1.91572712e-01\n",
            "  6.33997450e-01  4.27002984e-01  3.56635047e-01  4.53249466e-01\n",
            " -2.15956519e-01  3.65745601e-02  2.97473845e-01  1.46925544e-02\n",
            "  3.54239935e-04 -2.35788995e-01  3.01414820e-01  2.05208288e-01\n",
            " -1.42939491e-01  4.10563672e-01 -5.85003722e-02  1.32212898e-01\n",
            "  2.75558825e-01  4.01818049e-01  6.05759844e-01  3.96652076e-01\n",
            "  5.45508639e-02  2.24032556e-01  6.48492970e-02  5.08489559e-01\n",
            "  6.00041386e-01  2.72761197e-01  5.28106259e-01  5.95137076e-01\n",
            "  4.07832478e-02 -5.92614558e-02  4.98549327e-01 -1.84201617e-01\n",
            "  3.60418618e-01  1.85887532e-01  2.63792974e-01  9.65004376e-01\n",
            "  2.64601338e-01  1.51560810e-01  5.39717242e-01 -3.53028640e-02\n",
            "  2.67439723e-01 -6.35153966e-02  8.84250265e-02  3.86462628e-01\n",
            " -4.65550822e-02  3.58277996e-01  5.90729517e-01 -3.81157925e-02\n",
            "  1.44244386e-01  2.78853229e-01 -5.17741567e-02  7.74987755e-01\n",
            " -2.67126859e-01  4.60113996e-01  1.31977774e-01  4.62047834e-01\n",
            "  3.07683604e-01 -1.30903045e-01 -1.08240856e-01  1.50989922e-01\n",
            "  1.44050444e-01 -2.11680972e-01 -2.64638591e-01  2.39480120e-01\n",
            "  6.60198067e-01  4.96554233e-01  5.48174003e-02  9.38989464e-01\n",
            "  3.31698397e-01 -3.98433107e-03 -2.94049621e-01 -1.19216091e-01\n",
            "  2.94215239e-01  5.92757783e-01  3.78622447e-01  3.99559807e-01\n",
            " -6.93706965e-01  3.42938760e-01  1.36679901e-01  2.12576584e-01\n",
            "  6.20218168e-01  6.91249458e-02  2.91903654e-01  1.60123094e-01\n",
            " -1.57643342e-01  2.89674199e-01  1.06942778e-01  3.32486264e-01\n",
            "  2.02687577e-01  1.85592628e-01  2.55993698e-01  1.81146991e-01\n",
            "  4.51454954e-01  4.62814611e-01  2.15217897e-01  2.22922981e-01\n",
            "  2.72575496e-01  4.57742308e-01  2.54772817e-01  6.10980838e-02\n",
            "  8.61963484e-02  5.20887571e-01 -1.30653914e-01  6.23246296e-01\n",
            " -1.45072425e-02  4.30011479e-01  7.07437534e-02  2.69725777e-01\n",
            " -9.46932609e-02  1.01798272e-02  3.24012867e-03  3.64368298e-01\n",
            " -7.80792200e-02  5.70251547e-01 -1.64966016e-01  2.02927114e-01\n",
            " -1.67091963e-01  1.30729510e-01  4.65481047e-01  5.00816313e-02\n",
            " -7.78458574e-02  4.72715487e-01  8.70427735e-01  1.57186409e-01\n",
            "  2.13837124e-01  4.74159397e-01  1.08898452e-01  2.21062682e-01\n",
            "  2.75714302e-01  7.50196485e-02  6.62734686e-03  4.09877815e-01\n",
            "  2.66399676e-02  1.55001756e-01  5.77164827e-01  2.67249911e-01\n",
            " -1.05904801e-03  1.16320348e-01  6.97967075e-01  2.96000307e-01\n",
            "  9.41509271e-02  1.13814471e-01  4.51262957e-02  6.53282297e-01\n",
            "  5.15308956e-02  5.07466066e-01 -1.84848048e-01  1.78485579e-01\n",
            "  1.54827922e-01  2.51422144e-01  3.20196016e-01  2.61924150e-01\n",
            "  3.97160997e-02  5.53074848e-02  1.37083627e-01  4.18427847e-01\n",
            "  1.41506111e-01  4.06821525e-01  2.76729810e-01  5.02159374e-01\n",
            " -1.85400108e-01 -2.30732795e-01  8.75501349e-01  2.89847412e-02\n",
            "  5.59389221e-02  2.16519216e-01  2.08875713e-01  5.62869060e-01\n",
            "  4.49235347e-01  3.15291391e-01  1.55358219e-01  2.77732695e-01\n",
            "  1.51078116e-01  1.11318061e-01  4.35317602e-01 -1.96636043e-01\n",
            "  5.71522555e-01 -5.57209561e-02  2.28304140e-01  2.77900262e-02\n",
            "  3.97711377e-01  6.39980578e-02  3.04377627e-01  9.97845251e-02\n",
            "  4.36303843e-01  1.58270197e-01  1.82839176e-01  3.93152970e-01\n",
            "  1.46384090e-01  2.31340466e-01  7.25040051e-02  2.28353416e-01\n",
            "  5.81726228e-01 -3.33608358e-01  4.18993870e-01  2.42835423e-02\n",
            "  4.46481038e-01  2.15603567e-01  5.02657185e-01  6.97197751e-01\n",
            "  1.77345273e-01  3.94788527e-01  1.99770275e-01  2.88632824e-01\n",
            "  2.89533848e-01  5.31799649e-01  4.14656898e-03  1.31972775e-01\n",
            "  1.46070907e-01  2.01538768e-01  6.12300868e-01 -3.00993096e-01\n",
            "  1.31432824e-01  5.50672750e-01  1.20346894e+00  1.28122599e-01]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4e7xHvznXmQ",
        "colab_type": "text"
      },
      "source": [
        "## Random Tree Forest\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ie5R_xZdnVs1",
        "colab_type": "code",
        "outputId": "24b9186f-4284-4ed1-95f0-e3e09fd2b23d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "rf = RandomForestRegressor(n_estimators = 500, random_state = 777)\n",
        "rf.fit(mean_train_t, train_scores_t);\n",
        "predictions = torch.Tensor(rf.predict(mean_val_t))\n",
        "\n",
        "pearson = pearsonr(val_scores_t, predictions)\n",
        "mae  = mean_absolute_error(val_scores_t, predictions)\n",
        "print(f'RMSE: {rmse(predictions, val_scores_t)} Pearson {pearson[0]} MAE {mae}')\n",
        "print()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE: 0.8802624344825745 Pearson 0.2523580204246022 MAE 0.7092980742454529\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "so8O5gH7ISii",
        "colab_type": "text"
      },
      "source": [
        "## Simple FFNN with sklearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRKlhimuIVp7",
        "colab_type": "code",
        "outputId": "4bfa197e-44c7-46d6-8dbf-f2d30b84eadb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.neural_network import MLPRegressor\n",
        "from scipy.stats.stats import pearsonr\n",
        "\n",
        "\n",
        "mlpr_model = MLPRegressor(hidden_layer_sizes=(50,), batch_size=32, solver='sgd', learning_rate='adaptive', learning_rate_init=0.01, early_stopping=True)\n",
        "mlpr_model.fit(mean_train_t, train_scores_t)\n",
        "\n",
        "predictions = torch.tensor(mlpr_model.predict(mean_val_t))\n",
        "print(predictions)\n",
        "print(torch.var_mean(predictions))\n",
        "pearson = pearsonr(val_scores_t, predictions)\n",
        "mae  = mean_absolute_error(val_scores_t, predictions)\n",
        "print(f'RMSE: {rmse(predictions, val_scores_t)} Pearson {pearson[0]} MAE {mae}')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([-3.0276e-01, -9.4897e-02,  2.8105e-01, -2.9537e-01, -3.7779e-01,\n",
            "         7.4241e-01, -5.3593e-01,  2.1731e-01,  5.8119e-01,  4.8507e-01,\n",
            "        -2.7196e-01, -7.2199e-02, -3.7083e-01,  4.5651e-01, -4.4499e-01,\n",
            "        -4.8511e-01,  9.3002e-01,  5.4222e-01,  6.6252e-03, -2.2299e-01,\n",
            "         6.8237e-02,  4.5647e-02,  4.1723e-02, -1.5052e-01, -5.8786e-02,\n",
            "         2.2627e-01,  2.7944e-01,  3.6873e-01,  2.1625e-01, -1.9149e-01,\n",
            "         1.0953e-01, -7.5491e-01, -6.7379e-01,  4.2473e-01,  6.9424e-01,\n",
            "         6.5938e-01, -3.1465e-01,  5.0444e-03, -1.5383e-01, -2.0142e-01,\n",
            "        -1.3576e-01,  1.9079e-01, -2.1217e-02,  4.0349e-01,  4.6571e-02,\n",
            "        -2.5732e-02, -9.2815e-01, -1.0448e-01,  5.2452e-02,  1.5903e-01,\n",
            "         7.9887e-01,  4.9013e-01,  2.5842e-01, -2.7954e-01,  2.0021e-01,\n",
            "         3.5936e-02,  1.2197e-01, -3.4765e-01, -3.2995e-01,  1.1234e-01,\n",
            "        -5.3899e-04,  2.3027e-01,  5.1485e-01, -4.5353e-01,  3.4784e-01,\n",
            "         1.3395e-01,  4.1402e-01,  2.1461e-01, -6.9031e-02, -2.3234e-01,\n",
            "        -1.5509e-01, -1.3362e-01, -9.7117e-02, -1.3457e-01,  3.3142e-01,\n",
            "         3.5706e-01, -1.2507e-01,  4.1557e-01,  1.5307e-01, -3.4662e-01,\n",
            "         3.6011e-01, -5.7086e-02, -7.8290e-02, -4.9763e-01,  3.8505e-02,\n",
            "         2.1544e-01, -4.2333e-02,  1.6086e-01, -9.5484e-03, -1.7771e-01,\n",
            "        -1.0046e-01,  2.8363e-01,  9.6999e-01,  5.5655e-01, -3.2956e-02,\n",
            "         1.4493e-01, -2.8794e-01,  1.0794e-01,  2.3754e-01,  4.2411e-02,\n",
            "         3.2248e-01,  4.2611e-01,  4.3572e-01, -5.2280e-01,  3.1833e-01,\n",
            "         6.7212e-02, -4.4412e-01,  2.8777e-02, -3.2458e-01, -2.7475e-01,\n",
            "         9.5589e-02, -4.1802e-02,  3.2999e-01,  5.1167e-01,  3.0316e-01,\n",
            "         1.4479e-02,  1.8001e-01,  3.5190e-02,  5.9493e-02,  2.6673e-01,\n",
            "        -1.7701e-01, -5.1780e-01,  1.3967e-01, -5.2279e-02,  7.6252e-02,\n",
            "        -2.1229e-01,  5.5171e-02,  3.9986e-02,  3.7216e-01,  3.2739e-01,\n",
            "         2.3970e-01,  3.6743e-01,  3.8553e-02, -5.0009e-01,  2.6312e-01,\n",
            "         1.2449e-01, -5.2730e-02,  2.3401e-01,  5.1918e-01,  3.4112e-02,\n",
            "        -3.2823e-03,  2.6698e-01,  2.5827e-01,  1.2287e-01,  2.4411e-01,\n",
            "         2.0810e-01,  7.2983e-02,  1.1119e-01, -4.2510e-02,  1.1306e-01,\n",
            "         1.1932e-01,  4.4403e-01,  8.2997e-02, -1.1570e-01,  7.7662e-01,\n",
            "        -2.6811e-01,  2.0266e-01,  6.6604e-02, -2.0111e-02, -3.9614e-03,\n",
            "         1.6587e-01,  3.5514e-01, -1.4508e-01,  2.0241e-01,  1.4125e-01,\n",
            "         3.6820e-02,  1.8231e-01,  3.2877e-01,  6.8682e-01,  3.4952e-01,\n",
            "         6.7656e-03,  1.4204e+00,  5.5978e-02,  3.1084e-01, -1.3833e-01,\n",
            "         3.9317e-01,  1.3118e-01,  4.8628e-01, -7.7292e-01, -1.2261e-01,\n",
            "         5.0561e-01,  7.6082e-02,  9.4612e-02, -2.4722e-01, -5.9685e-01,\n",
            "         5.9250e-01, -3.6543e-01, -5.5564e-01,  5.0478e-02,  4.1771e-01,\n",
            "        -5.5204e-01,  9.4474e-01, -4.7140e-02,  2.0880e-01, -7.0366e-02,\n",
            "        -7.0145e-02, -2.2357e-02,  4.0996e-01,  3.6375e-01,  1.3421e-02,\n",
            "        -8.0766e-01,  3.3907e-01, -1.8064e-02, -2.8161e-01,  4.5721e-01,\n",
            "        -4.6485e-02, -3.2358e-01,  5.9957e-01, -4.4731e-02,  1.0992e-01,\n",
            "         1.2036e-01,  6.6426e-02, -1.1671e-01,  2.0922e-01,  2.1941e-01,\n",
            "         1.8784e-01,  6.2663e-02,  4.8224e-01, -6.9578e-02, -1.6566e-01,\n",
            "        -2.3926e-01, -1.4832e-01,  9.2510e-02,  4.7222e-01, -7.8375e-02,\n",
            "         1.0154e-01,  3.6858e-01, -1.1201e-01,  6.0913e-01,  2.6684e-01,\n",
            "         4.5801e-01,  3.8993e-01, -4.6996e-01, -1.2308e-01,  2.4021e-01,\n",
            "         6.7805e-02, -4.9552e-01, -3.9090e-01,  5.3929e-02,  4.5144e-01,\n",
            "         4.3519e-01,  8.0871e-01,  3.3984e-01,  3.0603e-01, -6.0767e-01,\n",
            "        -2.4739e-01,  6.1997e-02, -2.7178e-01,  2.3379e-01,  7.1095e-01,\n",
            "        -4.9722e-01,  4.4750e-01, -3.8184e-01, -8.4939e-01, -2.4987e-01,\n",
            "         3.3230e-01,  5.8688e-01, -7.3826e-01,  8.8058e-02,  4.8944e-01,\n",
            "         1.3317e-01, -8.6941e-03,  4.0871e-01,  1.8999e-01,  4.7948e-01,\n",
            "         5.8097e-01,  8.2518e-01,  5.0168e-01, -5.3533e-01,  3.6386e-01,\n",
            "        -2.5014e-01,  4.9070e-01, -7.3259e-02, -4.1166e-01,  2.8772e-01,\n",
            "         1.5063e-01,  1.9123e-01, -4.9179e-02,  1.5967e-01,  2.2050e-01,\n",
            "         7.5621e-01, -1.6539e-01,  1.0188e-01, -4.9805e-01,  2.0952e-01,\n",
            "        -1.5367e-01,  1.2354e-02,  5.4406e-01,  3.5926e-01,  2.4445e-01,\n",
            "        -4.0303e-01, -3.4279e-01,  4.7905e-01,  1.9203e-01,  6.1585e-01,\n",
            "         4.9979e-01, -3.7624e-01,  2.5725e-01,  7.5681e-03,  5.1391e-02,\n",
            "         2.6453e-01,  4.0580e-02,  1.7947e-01,  4.1212e-01,  5.6527e-03,\n",
            "        -6.8446e-01, -2.0656e-01,  6.7596e-01,  7.1355e-02,  3.5786e-01,\n",
            "        -2.5689e-01,  3.1970e-01,  2.2327e-01, -5.4933e-02, -1.5599e-01,\n",
            "         2.3945e-01, -3.8954e-01,  6.0012e-02,  6.4845e-01,  2.1640e-01,\n",
            "         6.1363e-01, -1.4745e-01,  2.8651e-01,  3.5100e-01,  3.3579e-01,\n",
            "        -4.1517e-01,  2.1680e-01,  2.1646e-01,  4.9723e-01,  5.6370e-01,\n",
            "        -1.4771e-01, -2.3554e-01, -1.4218e-01,  4.2767e-01, -1.9199e-01,\n",
            "        -6.0032e-01,  1.5920e-01, -6.9880e-03, -2.6030e-01, -2.5080e-01,\n",
            "        -1.4033e+00,  6.7170e-01,  2.5116e-01,  2.2673e-01,  2.7003e-01,\n",
            "        -6.4972e-01, -1.7175e-01,  6.3793e-01,  1.0698e-01, -2.6070e-01,\n",
            "         1.4138e-01,  8.1336e-01, -1.8964e-01,  1.1480e-01,  3.5902e-01,\n",
            "         8.3245e-02, -1.0482e-01,  3.2892e-01, -3.2251e-02,  1.9275e-01,\n",
            "         1.3896e-01,  1.5892e-01, -7.3361e-02,  8.2737e-01,  9.7053e-02,\n",
            "        -3.8076e-01, -6.0719e-01,  5.3330e-02,  6.9787e-02,  2.1271e-02,\n",
            "         7.1583e-02, -2.7704e-01,  6.4425e-02,  1.1941e-01, -1.3716e-03,\n",
            "        -3.2431e-01,  7.2014e-01, -2.9694e-01,  3.5034e-01, -4.9796e-01,\n",
            "        -3.5683e-01,  9.1520e-02,  1.1230e-01,  8.9848e-03, -3.7307e-01,\n",
            "        -6.9282e-02,  1.5891e-03,  1.3483e-01, -5.3459e-01, -1.4255e-01,\n",
            "        -3.5061e-01,  5.8752e-01, -2.8848e-01,  4.1856e-01,  8.2321e-02,\n",
            "        -1.6414e-01,  1.7027e-01,  6.7403e-01,  3.3406e-01,  3.7570e-01,\n",
            "         2.0188e-01,  1.0429e-02,  5.8918e-01, -2.5163e-01, -2.7512e-02,\n",
            "         2.4313e-01,  3.3751e-01,  3.9282e-02, -3.7460e-01,  8.2430e-01,\n",
            "         3.9881e-01,  2.9450e-01,  5.1187e-01,  1.6285e-01,  4.5959e-02,\n",
            "         3.0388e-02,  7.0701e-01, -5.1804e-01,  3.2379e-01,  1.5984e-01,\n",
            "         3.5188e-01,  5.4676e-01, -2.2286e-01,  3.8948e-02,  4.8668e-01,\n",
            "        -2.4490e-01, -3.1389e-01, -1.7924e-03,  8.0519e-01,  4.6827e-01,\n",
            "        -2.1222e-01,  8.1202e-02, -2.0561e-01, -3.4638e-01, -3.0525e-02,\n",
            "        -9.5407e-02, -4.4367e-01, -1.3634e-01,  4.6638e-01, -3.6206e-02,\n",
            "        -9.4505e-01,  4.2361e-01, -1.8400e-01,  2.7017e-02,  1.1002e-01,\n",
            "         1.4861e-02, -5.9327e-02,  2.7649e-02,  1.1631e+00, -2.1295e-01,\n",
            "        -1.4229e-02, -6.9785e-02,  2.4487e-01,  1.5987e-02, -2.3399e-01,\n",
            "         4.7524e-01,  3.7078e-01, -4.0315e-01,  3.2127e-01,  1.6603e-02,\n",
            "         5.7376e-01, -5.5840e-01,  1.4366e-01,  2.2081e-01,  3.4167e-01,\n",
            "         1.4465e-01, -2.5179e-01,  1.6767e-01, -4.9710e-02, -7.7061e-01,\n",
            "         2.2277e-01,  6.9774e-01,  2.1220e-01, -3.8925e-02, -1.5595e-01,\n",
            "         7.2705e-01, -2.2244e-01,  2.4870e-01,  1.8749e-01, -2.6175e-02,\n",
            "        -4.4750e-01,  1.8031e-01, -2.7573e-02,  4.8779e-01, -1.2645e+00,\n",
            "         1.6067e-01,  4.4925e-01,  1.6842e-01,  2.5740e-02,  8.7073e-02,\n",
            "         1.3605e-01,  7.5386e-01,  1.6296e-01, -8.4780e-01,  6.9025e-02,\n",
            "        -1.6391e-01,  5.6398e-01,  6.9623e-01, -1.7255e-01,  8.2204e-02,\n",
            "        -9.7559e-02,  8.4428e-03,  3.4130e-01, -1.5667e-01,  8.5128e-01,\n",
            "         6.5167e-01,  4.7679e-01,  2.0300e-01,  1.1907e-01, -1.4861e-01,\n",
            "         1.0304e-01, -1.4145e-01,  2.1032e-01, -3.8577e-01, -9.1935e-01,\n",
            "        -3.7406e-01, -1.6784e-01, -1.8162e-01, -3.1369e-01,  4.2277e-01,\n",
            "         2.5831e-01, -2.8457e-02, -1.6745e-03,  4.9258e-01,  2.5774e-01,\n",
            "        -2.6560e-02, -1.6251e-01,  3.2138e-01,  4.3175e-02,  3.8953e-02,\n",
            "         4.4588e-01, -2.4446e-01,  3.9903e-01,  7.6578e-02,  1.8469e-01,\n",
            "         2.4456e-01, -7.6159e-02,  8.8434e-01,  3.3825e-01,  1.0279e-01,\n",
            "        -2.2398e-01,  1.2198e-01, -1.6250e-01,  5.1028e-02,  1.0387e-01,\n",
            "         1.7778e-01, -4.7447e-01,  2.4810e-01,  1.7628e-02, -1.5789e-01,\n",
            "         2.2724e-01,  2.0331e-02,  1.5580e-01,  2.5142e-02, -8.2606e-02,\n",
            "         1.5854e-01,  5.5178e-01, -3.1493e-01, -1.5946e-01,  2.2583e-01,\n",
            "         2.3149e-01,  3.5080e-02,  3.0894e-01, -6.8353e-01,  3.1329e-01,\n",
            "         8.3199e-01,  3.9625e-01, -2.5895e-01,  6.3954e-01, -2.7587e-02,\n",
            "         2.5345e-01,  6.2803e-01,  8.8094e-02,  4.9800e-01, -3.7321e-01,\n",
            "        -3.1613e-01, -1.4604e-01,  5.3262e-01, -7.4460e-02,  1.7530e-01,\n",
            "        -8.2722e-02,  2.3318e-01, -3.5505e-01,  3.9896e-02,  2.9946e-01,\n",
            "         3.6834e-01,  3.0584e-01,  4.7325e-01,  2.0867e-01, -6.6035e-01,\n",
            "         6.5373e-01, -3.0784e-01,  4.8911e-01,  3.5988e-01,  7.9998e-01,\n",
            "         7.1610e-02,  3.7551e-01,  3.8017e-01, -4.4501e-01,  3.0092e-01,\n",
            "         5.9015e-01,  2.2136e-01,  1.8547e-01,  3.5474e-02,  1.0433e-01,\n",
            "        -6.1941e-01,  7.0920e-01,  2.9240e-01,  3.9215e-01,  4.9833e-01,\n",
            "         4.5248e-01,  5.2888e-01, -5.6376e-01,  4.8069e-01,  5.3043e-01,\n",
            "         4.6293e-01,  3.5338e-01,  9.6621e-02, -1.4499e-01,  5.6116e-01,\n",
            "        -1.8635e-01, -4.1586e-02,  7.5440e-02, -1.6110e-01,  3.6802e-01,\n",
            "        -8.8256e-02, -3.0911e-01, -2.8520e-02, -2.4093e-01, -2.8055e-01,\n",
            "        -1.5728e-03, -1.9477e-01,  1.9889e-02,  7.2052e-02, -3.1683e-01,\n",
            "        -3.9098e-01,  5.6164e-01,  2.0471e-02,  4.1561e-01,  2.5663e-01,\n",
            "        -1.2291e+00,  1.6969e-01, -1.7818e-01,  5.6883e-01,  4.0331e-01,\n",
            "        -2.4721e-01,  2.5657e-01,  6.2501e-01,  2.8078e-01,  1.3704e-01,\n",
            "         4.2218e-01,  5.8881e-01,  3.7263e-01,  2.0178e-01,  2.4599e-01,\n",
            "         4.3054e-01, -6.3342e-02, -3.9951e-01,  1.5878e-01,  6.4755e-01,\n",
            "         8.0479e-02, -1.8760e-01, -1.6103e-02,  2.0178e-01, -3.5073e-01,\n",
            "         1.1539e-01,  5.1395e-02, -1.5057e-01,  9.0514e-01, -1.2457e-01,\n",
            "        -1.1241e+00,  1.1361e-01,  7.1383e-03, -6.4552e-01,  2.8904e-01,\n",
            "        -5.9701e-01, -2.4141e-02, -6.5038e-02,  4.2781e-01, -5.9847e-02,\n",
            "        -3.6252e-01,  4.3887e-01,  1.8322e-01,  1.9079e-02,  2.0739e-01,\n",
            "         3.3047e-01,  1.4027e-01,  4.1137e-01,  4.5948e-02, -2.1303e-01,\n",
            "         6.3755e-01, -1.9526e-01, -1.0099e-02,  7.0524e-01, -1.8663e-01,\n",
            "         3.1997e-01, -1.9747e-01,  2.1951e-01,  3.3474e-01, -1.3721e-01,\n",
            "        -1.4386e-01,  5.4620e-02,  7.8703e-02,  4.7903e-01,  1.2422e-01,\n",
            "        -5.9822e-02,  3.4866e-01,  4.3346e-01, -7.1683e-02,  3.4673e-01,\n",
            "         1.2800e-01, -4.6000e-03,  2.8556e-01,  2.3487e-01, -7.3907e-02,\n",
            "         9.3542e-02,  5.0603e-01, -1.7024e-02,  4.8681e-01, -2.9788e-01,\n",
            "        -2.3170e-01, -2.2952e-01, -2.0349e-01, -3.0254e-01,  1.9820e-01,\n",
            "         2.8111e-01, -2.6964e-02,  1.6905e-01,  2.6015e-01,  2.9433e-01,\n",
            "         8.2229e-01,  2.4297e-01, -1.1610e-01,  4.2749e-01, -1.2400e-01,\n",
            "         6.5183e-02,  1.4796e-01, -5.0984e-01, -2.4567e-02, -5.3322e-01,\n",
            "        -9.7454e-01,  9.1818e-01,  6.9883e-03, -1.6148e-01, -3.5031e-01,\n",
            "         5.4913e-01,  3.5451e-01, -2.2757e-01, -8.3572e-02,  8.2136e-02,\n",
            "         1.5762e-01, -5.3792e-01,  1.6685e-01,  1.9331e-01, -3.4739e-01,\n",
            "         1.7824e-01, -2.6786e-01,  1.3174e-01,  2.0070e-01,  2.4150e-01,\n",
            "         9.1405e-02, -4.9337e-01,  6.5440e-02,  3.7352e-01,  5.7851e-01,\n",
            "         5.7015e-01,  4.3686e-01, -2.8585e-01, -9.7142e-02, -1.5517e-01,\n",
            "         1.0586e-01,  2.5356e-02,  1.8093e-01,  5.9350e-01,  2.9440e-01,\n",
            "        -4.8719e-01, -4.3800e-03,  8.5800e-04,  1.8700e-01, -1.3211e-01,\n",
            "         1.9692e-01, -1.4183e-01,  3.4652e-01,  7.6248e-02,  7.5600e-02,\n",
            "         1.3778e-01,  2.2096e-01,  3.4323e-01,  7.7373e-01,  6.2850e-01,\n",
            "         3.0734e-02,  2.0550e-01,  4.2056e-01, -1.5122e-01, -7.6976e-02,\n",
            "         5.6410e-01, -6.1919e-03,  2.3201e-01, -2.0999e-01, -1.3181e+00,\n",
            "        -3.4618e-01,  4.8475e-01,  5.7277e-02,  6.0483e-02,  3.9733e-01,\n",
            "        -2.5404e-02,  1.4686e-02, -7.7026e-02, -3.7354e-02,  5.6584e-01,\n",
            "         8.0395e-01, -7.3131e-02,  5.3832e-01, -9.9830e-02, -4.9443e-01,\n",
            "         5.3562e-01, -6.8804e-02,  2.8790e-01, -4.0770e-01,  3.8368e-01,\n",
            "         4.8771e-02, -9.8277e-02,  2.0188e-01,  1.4036e-01,  1.4470e-01,\n",
            "         1.8606e-02, -6.4670e-02,  1.9347e-01,  3.0715e-01,  2.3481e-01,\n",
            "        -1.6276e-02,  1.8680e-01, -2.3233e-01,  3.6030e-01,  3.2586e-02,\n",
            "         2.3593e-01, -2.3279e-01, -5.3826e-02, -1.8379e-01,  1.9512e-01,\n",
            "         9.4287e-02,  1.5204e-01,  2.5226e-01,  2.0268e-02,  6.9070e-02,\n",
            "         9.6215e-01,  1.5006e-01, -3.9966e-01,  1.2405e-02,  3.3911e-01,\n",
            "         2.0795e-01, -3.6580e-01,  7.1926e-02, -1.4584e-01,  7.4726e-02,\n",
            "        -2.5760e-02,  4.3959e-03,  1.3467e-01, -8.9474e-02,  1.6089e-01,\n",
            "         3.0605e-02,  2.7946e-02,  1.6008e-01,  1.8866e-01,  1.7141e-01,\n",
            "        -1.2534e-01, -7.8139e-02,  7.1923e-02, -1.5709e-01,  3.3918e-01,\n",
            "        -4.5137e-02, -1.7576e-02,  3.0569e-01, -1.1314e-01,  1.3156e-01,\n",
            "         9.5813e-01,  1.9765e-01,  6.2660e-02, -2.0687e-01,  6.5718e-01,\n",
            "        -7.7463e-02,  6.8108e-01, -8.0751e-02,  1.2814e-01,  2.4367e-01,\n",
            "         1.2296e-01,  1.6934e-01,  3.1735e-02,  3.2982e-02, -1.9682e-01,\n",
            "         6.0203e-02, -3.9692e-01,  3.4905e-01, -9.5700e-03,  6.5276e-01,\n",
            "         3.3579e-02, -4.1650e-01,  4.3708e-01, -3.1263e-01,  9.7114e-02,\n",
            "        -1.4042e-01,  2.1638e-02,  3.1393e-01,  6.1073e-01, -6.2040e-02,\n",
            "         4.0440e-01, -3.3508e-01, -1.4478e-01, -1.9459e-01, -2.9918e-01,\n",
            "         6.6491e-01,  4.7629e-01,  1.0663e-02, -1.9057e-01,  1.2411e-01,\n",
            "         2.1037e-02,  3.0325e-01,  1.0740e-01, -9.4715e-02, -5.0711e-02,\n",
            "         2.1633e-02, -2.3045e-01, -9.0525e-02, -4.0346e-02,  1.1954e+00,\n",
            "         1.3161e-01, -2.0869e-01,  1.5626e-01,  1.4842e-01,  1.2959e-01,\n",
            "         1.1327e-01, -4.6809e-02, -3.5466e-01, -4.5955e-02, -2.8318e-01,\n",
            "         3.3182e-01,  2.0316e-01, -1.6642e-01,  1.3657e-02,  4.2206e-01,\n",
            "         3.8782e-01,  3.4356e-01, -7.6383e-01,  1.4658e-02,  1.0446e-01,\n",
            "        -1.6833e-01, -1.7699e-01, -2.2533e-02,  3.0557e-01, -7.0309e-02,\n",
            "         5.1569e-01,  5.9679e-02, -2.6159e-01,  4.8011e-01,  2.8109e-01,\n",
            "        -3.9487e-01,  7.3582e-02, -1.5363e-02, -2.6971e-01, -1.9551e-01,\n",
            "         7.7455e-01,  1.7836e-01, -1.1737e-01,  2.8783e-01, -1.2789e-01,\n",
            "        -2.5944e-02, -2.0214e-01, -1.8524e-01,  4.4991e-01,  1.4179e-02,\n",
            "         2.2939e-01, -1.2532e-01, -1.0555e-01, -2.6237e-01, -1.2949e-01,\n",
            "         7.5978e-01,  2.0090e-01, -1.5047e-01,  1.9350e-01, -2.8797e-01,\n",
            "         3.0136e-01, -6.2046e-01, -4.6057e-01, -5.6373e-01,  8.1521e-03,\n",
            "         5.3304e-01, -3.5755e-01,  1.7276e-01, -2.3562e-01, -3.8066e-02,\n",
            "         1.1195e-01, -7.6279e-02, -2.6503e-01,  1.4432e-01,  5.0969e-01,\n",
            "         4.4054e-01,  2.7029e-01,  8.8940e-01, -9.9985e-02,  4.7712e-02],\n",
            "       dtype=torch.float64)\n",
            "(tensor(0.1215, dtype=torch.float64), tensor(0.0837, dtype=torch.float64))\n",
            "RMSE: 0.8757208959234649 Pearson 0.30386372630915565 MAE 0.6704600217694316\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFGkpC8HPM0L",
        "colab_type": "code",
        "outputId": "b53a6dd2-f631-4fe1-e56a-21e94012d6f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "mlpr_model = MLPRegressor(hidden_layer_sizes=(50,), batch_size=32, solver='sgd', learning_rate='adaptive', learning_rate_init=0.01, early_stopping=True)\n",
        "mlpr_model.fit(mean_train_val_t, train_val_scores_t)\n",
        "\n",
        "predictions = mlpr_model.predict(mean_test_t)\n",
        "print(predictions)\n",
        "\n",
        "writeScores(predictions, 'FFNN.txt')\n",
        "files.download('FFNN.txt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 2.10889232e-02 -7.60931838e-02  7.43149609e-01 -1.38666407e-02\n",
            " -4.23213237e-01 -2.63102570e-01 -6.89221848e-01  2.59093571e-01\n",
            " -1.40938215e-01 -2.91391971e-02  9.02422338e-01 -6.17110392e-01\n",
            " -3.00688367e-01 -4.98844642e-01 -1.03486218e-01  1.15930911e-01\n",
            " -2.85600752e-02 -3.78749727e-01 -1.91574493e-01  2.09574557e-01\n",
            " -4.78055960e-01  2.66993229e-01 -1.22469746e-01  7.23622502e-01\n",
            " -1.22150925e-01 -3.85848283e-01  2.31523218e-01  6.38018315e-01\n",
            " -3.94990415e-02 -7.59014040e-02 -2.18929507e-01 -4.83231935e-02\n",
            " -4.76917700e-01 -1.02907972e-01  1.24083786e-01 -3.02937976e-01\n",
            " -3.06348548e-01 -2.12300965e-01  7.14254602e-02  3.01063101e-01\n",
            "  1.71962415e-01 -7.07028578e-02  2.41537699e-01 -3.02225065e-02\n",
            " -7.27219379e-02  4.03575073e-01 -5.08590199e-01  9.58694378e-02\n",
            " -4.84927063e-01 -1.78342424e-01  1.89810587e-01  1.89827423e-01\n",
            " -3.64909352e-01 -5.69975245e-01 -3.34035953e-01 -8.32102674e-02\n",
            " -1.13434782e+00 -2.42537174e-01  3.27560205e-02  2.72445215e-01\n",
            " -2.15320975e-01  6.53788158e-01  5.16915662e-02  1.13140538e-01\n",
            "  1.54357996e-01  5.08026571e-01 -1.18995451e-01 -1.75707585e-01\n",
            " -4.34725697e-01  5.27214416e-01 -3.91834550e-01  1.04241994e-01\n",
            " -1.01763222e-01  1.88616755e-01 -4.67479555e-01 -1.39230466e-01\n",
            " -3.91016634e-01 -2.77064702e-01 -3.00110620e-01 -2.75472678e-01\n",
            "  5.98671562e-02 -2.15343041e-01 -1.02317214e-01 -1.67454152e-01\n",
            "  7.06650929e-02  1.87967252e-01  3.73004726e-01 -4.94202965e-01\n",
            " -3.67035869e-04  6.37505692e-01 -5.72797985e-02 -4.11219305e-01\n",
            "  2.28765833e-01  1.86613523e-01 -1.52313145e-01  2.50782675e-01\n",
            "  3.83343840e-01  6.84913364e-02 -9.03523533e-01 -9.78784019e-02\n",
            " -1.27817420e-01  1.56551869e-01  1.12922643e+00  4.89564909e-02\n",
            "  2.47023945e-01  1.40254693e-01  1.50214295e-01 -1.30485051e-01\n",
            "  5.31068197e-04  5.54631408e-02 -1.18976757e-01 -1.96851719e-01\n",
            "  1.04636346e-01 -4.52416919e-01  2.29650816e-01 -3.08666872e-01\n",
            "  1.03469232e+00 -1.64021755e-01  1.96650445e-01 -8.92535071e-01\n",
            "  1.72966497e-01  4.13755728e-02 -2.43671250e-02 -4.95195557e-01\n",
            " -1.12457216e-02  2.17700340e-01  4.01480091e-01 -7.73861777e-01\n",
            "  4.10704751e-01 -4.17278871e-02  4.58133441e-01 -2.18471941e-01\n",
            " -9.67312966e-02 -4.06346523e-01  1.64174663e-01  4.21209306e-01\n",
            " -1.89333004e-01 -2.24431221e-01  1.90454205e-01 -1.23817725e-02\n",
            " -8.87518378e-01 -9.23309939e-02  5.96822269e-02 -5.13534526e-02\n",
            "  1.41574788e-01  4.78184420e-01  1.66186134e-01 -1.03290614e-01\n",
            " -4.53505207e-01 -2.15318490e-01 -9.80265126e-01 -2.65391522e-01\n",
            " -2.64942310e-01 -1.06041350e-01 -2.22928997e+00 -5.07543924e-01\n",
            "  2.02901359e-01  4.49864316e-01  4.24374727e-01  4.43896691e-01\n",
            " -1.67394764e-01 -4.33946455e-01 -2.76514639e-01 -6.75099506e-02\n",
            "  1.25954965e-01  8.23884176e-02 -4.96969046e-02  3.17025914e-01\n",
            "  1.21971526e-01  7.90612720e-02  5.24878028e-01  4.61127625e-01\n",
            "  2.99770202e-02  5.26469875e-01  7.04646387e-02  1.51144088e-01\n",
            " -8.76820047e-02 -1.50567266e-01 -2.71593032e-01  1.74242685e-01\n",
            " -3.07622327e-01  2.49327917e-01  4.04883348e-01 -8.62617696e-01\n",
            " -3.36094545e-02 -1.22090070e+00  1.46555820e-01 -3.42799840e-01\n",
            "  1.52545270e-01  6.77644380e-02 -4.15556149e-01 -4.17463190e-01\n",
            " -1.05630496e-01 -2.15157710e-01 -2.10114445e-01 -2.17278021e-02\n",
            "  5.68116620e-01  1.69355451e-01  4.05297323e-01  5.74341979e-01\n",
            " -1.93938933e-01  6.80766466e-01  5.60679540e-01  2.42952566e-01\n",
            " -3.44132545e-01 -1.13980349e-01  1.17051364e-01  2.33601936e-01\n",
            " -4.53818437e-01 -1.22077856e-01  4.87921627e-02  3.97808393e-01\n",
            " -1.32795601e-01 -6.00695947e-01 -3.87669034e-01  3.68312168e-01\n",
            " -9.86639803e-02 -6.35525318e-01 -2.27283513e-01  1.52452937e-01\n",
            "  8.75204173e-02  1.38283394e-01 -2.81869864e-01 -3.45552530e-01\n",
            " -1.55353256e-02  4.08838034e-02 -2.50495910e-01 -2.47042528e-02\n",
            "  8.72735356e-01 -5.67442563e-01  3.49018307e-01 -4.76598838e-02\n",
            " -4.45979897e-02  4.82964085e-02  3.22400826e-01 -5.93666475e-02\n",
            "  3.72431658e-02  2.68873932e-02  2.50495974e-02  2.79418441e-01\n",
            " -9.34087332e-02  8.76816990e-01  3.15671084e-01  1.88893409e-01\n",
            " -2.03378466e-01  5.09923919e-01 -2.39504598e-01 -9.81672085e-02\n",
            " -1.34417535e-01 -3.11131885e-01 -6.05414134e-01  3.50346588e-01\n",
            "  3.36178258e-01 -1.58233283e-01 -2.60873473e-01 -2.52324449e-01\n",
            "  1.05718812e-02 -4.63450007e-01 -2.94151596e-01 -2.12038572e-01\n",
            " -2.16610776e-01 -6.43714484e-02  7.04510541e-02  7.83186726e-02\n",
            "  8.54316641e-01  2.90435213e-02 -4.14586321e-01 -6.63169992e-01\n",
            " -4.30194533e-02  1.37677982e-01  2.87765543e-01 -1.17365399e-01\n",
            "  6.30833748e-01 -5.36570716e-01  4.36822702e-01  3.17797944e-01\n",
            " -9.43648482e-02  5.31954261e-01  2.15087359e-01 -1.29447899e-01\n",
            "  5.53510319e-01 -4.28677730e-01  6.19341956e-01  1.45485372e-01\n",
            "  2.44269160e-02 -4.00774999e-01 -2.02066443e-02 -9.80023961e-02\n",
            " -7.02692612e-02  5.19298532e-01  3.58287401e-01 -4.84470386e-01\n",
            "  3.95973542e-01  9.38242027e-02  6.58900926e-01 -1.30674065e-01\n",
            " -4.78334962e-01 -5.59199741e-02  3.30975172e-01 -1.73803079e-01\n",
            " -2.92509214e-01 -1.77863202e-01 -3.48433360e-01  1.41668150e-01\n",
            "  8.93469458e-01  4.96469546e-01  4.17201478e-01  3.82081242e-01\n",
            "  2.34319888e-01 -5.32402506e-01  1.43368899e-01 -5.06464602e-01\n",
            "  1.01864392e-01 -4.82393465e-01  1.47798190e-01 -3.11393306e-01\n",
            " -7.62543485e-02  8.33167717e-02  3.45719379e-01  2.12334986e-01\n",
            "  3.37872330e-01 -6.14764336e-03  3.21082425e-01  3.47588802e-01\n",
            "  6.14101047e-01  4.45009636e-01 -5.33121217e-03  2.79602821e-01\n",
            "  1.96900775e-01 -3.20766888e-02 -3.39400725e-02 -5.07247722e-01\n",
            "  2.53713987e-01 -2.15269019e-01  8.58145053e-02 -7.55790408e-01\n",
            " -4.85681609e-01 -1.46448769e-02  1.68834713e-01 -9.10547350e-01\n",
            "  1.21801841e-02  4.19613940e-01 -1.53168723e-02  3.05516246e-01\n",
            " -1.33883890e-01 -6.83701245e-01 -1.29035553e-01 -8.00315805e-02\n",
            " -3.49095993e-01  1.90343907e-01 -1.00860874e-02 -8.32593659e-03\n",
            " -1.10359415e-01 -3.20419633e-01 -4.97099507e-01 -2.11346279e-01\n",
            "  1.44253819e-01  1.13039303e-02  5.69439232e-01 -1.10895657e-02\n",
            "  6.79117898e-01 -1.19468738e-01  1.38041714e-01 -2.17943123e-01\n",
            "  2.55295875e-01  6.86276595e-01 -7.24235630e-01 -8.25851239e-01\n",
            " -2.53619377e-02 -4.87477899e-01 -1.28644269e-01  8.42003111e-02\n",
            "  1.47425746e-01  3.39504327e-01 -2.59859176e-02 -2.26523711e-01\n",
            " -2.31074060e-01 -8.54831164e-03 -1.81725300e-01  1.51907005e-02\n",
            " -7.13595542e-01  1.19566667e+00  4.41465117e-01 -1.59919577e-01\n",
            " -1.45693796e-01 -4.20796035e-01 -2.56038239e-01  4.86377571e-01\n",
            " -4.74705427e-01  6.04509216e-01 -1.65140873e-01  3.08198898e-01\n",
            "  1.79850905e-01  1.09396804e-01  1.06245110e+00  3.89312492e-01\n",
            "  3.57134147e-02  2.05948393e-01  1.75780645e-01 -5.87099329e-01\n",
            " -1.48293388e-01  2.28005947e-01  3.10556613e-01 -1.84726927e-04\n",
            "  6.98236473e-01 -4.98676012e-01  6.76618719e-01  2.53727109e-01\n",
            " -3.64843793e-01 -7.09730147e-01  5.13507317e-01 -2.76637463e-01\n",
            " -2.22287413e-02 -3.06460368e-01  8.78978006e-02 -4.14212016e-01\n",
            "  6.66573191e-01  2.01843189e-01 -5.88978809e-01  2.75975461e-02\n",
            " -1.98846095e-01 -1.43059391e-01  3.48850620e-01  1.32880844e-01\n",
            " -1.12615280e-01 -1.22429489e-01 -6.15695186e-01  2.90040187e-01\n",
            " -5.27018409e-01 -2.02640088e-01 -6.73183794e-01 -1.75517541e-01\n",
            " -1.78945657e-01 -7.90206876e-02  1.47922880e-01 -1.72265518e-02\n",
            "  7.92429848e-02 -3.98816615e-01  1.19725005e-02 -8.29478459e-04\n",
            " -1.78752214e-01  1.26878410e-01 -2.33626835e-01 -3.87187026e-02\n",
            "  3.83708570e-02 -1.44788117e-01  1.93165907e-01  7.46163945e-02\n",
            "  5.95786593e-01 -5.25780578e-02  2.01500742e-01 -3.11958815e-01\n",
            "  2.96190756e-01 -1.08998886e+00 -2.57277467e-01  5.01522357e-02\n",
            " -1.96874140e-02  5.73537117e-01 -1.54772062e-01  2.46617429e-02\n",
            "  2.92019028e-01  2.15164249e-02  1.29859998e+00  4.90685700e-01\n",
            "  1.23572643e-01  3.96865208e-01  1.80102416e-01 -2.74357041e-01\n",
            "  5.12832664e-01 -1.83838951e-01 -9.93444878e-02 -1.61005254e-01\n",
            " -2.61129343e-01 -1.36335495e-01  1.60255616e-01 -3.50414152e-01\n",
            " -1.13146140e-01 -1.98687123e-01  2.10009063e-01  9.17447719e-02\n",
            " -4.21767414e-02  7.90030155e-01 -1.67389963e-01  4.80475930e-01\n",
            " -3.03172359e-01  1.10472130e-01  8.00293776e-01 -2.78645156e-01\n",
            " -3.16542767e-01 -2.40526788e-01  4.06818255e-01 -8.41099606e-02\n",
            "  4.97619646e-01  3.54275326e-01 -1.05842934e-01  9.66811649e-02\n",
            " -1.28206321e-01  1.81699153e-01 -8.99810190e-02 -1.88973332e-01\n",
            "  1.32257948e-01 -1.66486714e-02 -1.59597334e-01 -1.69100001e-01\n",
            "  2.74927129e-02 -1.03300216e-02  4.74451398e-01 -6.46817456e-02\n",
            " -2.83118102e-01 -3.09872404e-01  2.26505231e-01 -4.66121556e-01\n",
            "  3.96589974e-01  1.90298099e-01 -8.70597881e-02 -2.64855160e-01\n",
            " -1.60504979e-02 -3.50462655e-01  2.76258748e-01  3.19731736e-01\n",
            "  3.08545454e-01 -1.04920962e-01 -2.41075689e-01  1.20776666e-01\n",
            " -4.77394111e-02  1.72085305e-02 -1.41935660e-01  9.84036231e-02\n",
            "  9.76730994e-02  7.92534738e-02  4.92693687e-01 -1.94187995e-01\n",
            "  3.48490066e-01 -2.39611279e-01 -1.61774042e-01  7.20975333e-02\n",
            " -1.57011230e-02 -1.80683615e-01 -3.10762053e-01  1.46157308e-01\n",
            " -2.03072277e-01  2.46484767e-01  7.50246760e-02 -2.48541959e-01\n",
            "  1.61108555e-01 -1.60905946e-01 -3.46609686e-01 -3.15471723e-01\n",
            " -3.86557651e-02 -5.55422697e-01  8.47675215e-02 -4.06393644e-01\n",
            " -4.61579859e-01  2.44135015e-02 -5.00205891e-01 -5.26629999e-01\n",
            "  1.19574826e+00  2.47904722e-01  3.52927617e-01 -2.63621302e-02\n",
            "  1.73814963e-01 -2.19601144e-01  8.81905799e-01  2.92319722e-01\n",
            "  1.08424991e-01 -2.14690162e-01  8.06877179e-02  8.38014115e-02\n",
            "  2.15704260e-01  4.72494576e-02 -2.81503974e-01  2.21085759e-01\n",
            " -2.60804251e-01  1.23148461e+00 -2.20724311e-01  3.00885612e-01\n",
            "  3.11210521e-01 -2.82181025e-01  1.15538904e-01 -2.79168999e-01\n",
            "  5.39533588e-01 -3.19620362e-01 -7.81137041e-03 -4.08581409e-01\n",
            " -2.21810366e-01 -6.61695462e-02  4.47315010e-01 -1.07896586e-01\n",
            "  1.19223542e+00 -1.32685686e-01 -1.29891948e-01 -6.89682542e-01\n",
            " -2.06926134e-01 -1.75130131e-01 -1.97600377e-01  3.85626366e-01\n",
            " -8.27932893e-01  1.26938675e-01 -1.34751790e-01  4.55035198e-01\n",
            " -3.68813589e-01  5.04712809e-01 -5.59768296e-02 -1.53402630e-01\n",
            " -5.14568508e-01  1.30419477e-01  3.42007546e-01  4.13249300e-01\n",
            "  5.77405505e-01  6.04370608e-01 -4.84512165e-01 -1.43781421e-01\n",
            " -4.24926671e-02  1.73513000e-01 -3.06803248e-03 -1.14590667e-01\n",
            " -4.35432342e-01  1.31062118e-01 -1.82356014e-01  2.60710350e-02\n",
            " -4.20455741e-01  4.07082747e-01 -1.76068735e-01  5.47671713e-01\n",
            "  2.95873487e-01  1.69379872e-01  2.29488057e-01  7.75393922e-02\n",
            " -1.92920389e-02  3.56334825e-01 -6.41432195e-01 -8.01011940e-02\n",
            " -1.60853127e-01 -4.90822987e-01  8.47784948e-01  4.64439297e-02\n",
            " -2.78770469e-02 -2.59607667e-01  4.13097517e-01 -2.32930331e-01\n",
            "  2.28367165e-01  6.24993659e-03  6.88255058e-02  3.51462126e-01\n",
            " -2.25936627e-01 -4.95284820e-01 -1.61455786e-01 -1.09680576e-01\n",
            " -1.03087767e+00  6.80440290e-02  1.07718490e+00 -2.18694180e-01\n",
            " -6.57684206e-01 -1.89524595e-01 -3.93944855e-01 -1.28776615e-01\n",
            " -1.68023104e-01 -2.14560173e-02 -1.14553323e-01 -3.73735688e-02\n",
            " -2.88846499e-01 -2.43102079e-01 -1.96951479e-01  6.44114011e-02\n",
            "  1.14990619e-01  8.37525758e-01  4.15723268e-01 -5.35216829e-02\n",
            "  2.73390530e-01 -2.03573929e-01  9.89925150e-02  1.57852368e-01\n",
            "  4.26467625e-01  6.37567666e-01  4.99490976e-01 -2.04469534e-01\n",
            " -1.72694955e-01  3.71111203e-01  3.10865096e-01 -1.35925477e-02\n",
            " -2.86565668e-01 -5.54537522e-02 -3.54567831e-02  5.56598120e-01\n",
            "  4.68889174e-01 -1.33413385e-01  1.41272636e-01  1.64993224e-01\n",
            " -4.90671093e-01  2.42493925e-01 -3.44360144e-01  3.22553595e-02\n",
            " -2.53032601e-01 -9.60290184e-02  2.56530178e-01 -4.21681776e-01\n",
            "  2.55354176e-01  1.97795896e-01 -2.86687185e-01  3.57375097e-01\n",
            "  5.91400837e-02 -3.13903801e-01  6.28562470e-03  9.25547939e-01\n",
            " -2.93986739e-01 -1.87861110e-01 -1.55929822e-01  2.42693130e-01\n",
            "  1.91519588e-02  5.69701434e-01 -6.90611740e-01 -4.20783024e-01\n",
            "  3.31719520e-01 -5.43279234e-01  1.80800792e-01 -5.03274757e-01\n",
            "  5.06773500e-01  6.08662361e-01  2.81629099e-01 -5.96069961e-01\n",
            " -4.30177127e-01 -4.38543035e-01 -1.06991874e-01  8.18497467e-01\n",
            "  3.94210810e-01 -3.65901180e-01 -6.25363451e-01 -7.58650323e-01\n",
            " -1.71361977e-01 -5.65978112e-01 -2.16971507e-03  2.81523386e-01\n",
            "  4.27080769e-01 -3.40926299e-02  1.16575876e-01 -3.86460390e-02\n",
            " -4.32290922e-01  3.73668907e-01 -6.28670359e-01  3.78457364e-01\n",
            " -2.52927602e-01  2.50140822e-01 -2.06889802e-01 -2.24663783e-01\n",
            " -2.36208928e-01 -4.38843115e-01 -3.51212731e-01  7.45252838e-01\n",
            " -2.91085784e-01 -1.10080512e-01 -4.55942397e-01  1.28025698e-01\n",
            " -7.18147104e-01  3.40540604e-02  1.74490087e-01 -6.96526400e-02\n",
            "  1.73303027e-02  3.05341180e-01  2.39391628e-01  3.08594914e-01\n",
            "  4.09511043e-01 -3.23138739e-01  5.83850723e-01  6.36759525e-01\n",
            " -3.13596292e-01 -4.86540286e-01 -9.31794419e-02 -9.14952579e-03\n",
            "  2.77264360e-01  2.50398221e-02  1.92659390e-01  4.91536401e-01\n",
            "  1.79184909e-02  1.45066290e-02 -1.66025561e-01  3.12273939e-01\n",
            "  6.78505086e-01 -2.60812730e-01  2.08174622e-01 -3.12682609e-02\n",
            "  5.56546560e-01  3.01040760e-01  2.42076234e-01 -1.74199706e-01\n",
            "  5.33027710e-01  5.93846463e-01 -1.71279130e-01  3.25246378e-01\n",
            " -1.39546596e+00 -2.11206938e-01  4.21592518e-01 -3.25169070e-01\n",
            " -3.14483452e-01 -3.86150899e-01  2.59944334e-01 -2.23950397e-01\n",
            " -3.39434360e-01  2.02570068e-01 -2.54888297e-01  4.00650951e-02\n",
            "  3.61272947e-01  2.68923626e-01  6.38100688e-01  1.79306338e-01\n",
            "  3.71888484e-02 -6.92265786e-02 -2.49370302e-01  1.59238479e-01\n",
            "  2.26105212e-01 -5.89952639e-01  4.47049578e-01  6.14145807e-01\n",
            " -3.11899427e-01 -4.73821534e-01  2.44936560e-01 -4.72373267e-01\n",
            "  1.52369338e-01  1.91360425e-02 -4.35331234e-02  1.10134568e+00\n",
            "  8.71675279e-02  1.41298577e-02  2.10123212e-01 -5.49344963e-01\n",
            " -2.92125665e-02 -5.61653338e-01 -1.67185880e-01  1.50769083e-01\n",
            " -2.71596495e-01  2.41464859e-01  1.94447906e-01 -1.57527598e-01\n",
            " -2.12165915e-01 -5.93210880e-02 -4.99913430e-01  8.17805167e-01\n",
            " -5.46452936e-01  4.00998966e-01 -1.44307249e-01  7.64670934e-01\n",
            "  1.01862162e-01 -2.78180703e-01 -3.73109128e-01 -2.74626234e-01\n",
            " -9.39915247e-02 -1.54370182e-01 -4.17742942e-01  1.62858216e-02\n",
            "  4.76345410e-01 -1.33383939e-01 -4.48410585e-01  8.16197710e-01\n",
            "  3.08836451e-01 -2.85855880e-01 -4.67092504e-01 -8.19200613e-01\n",
            "  2.00215054e-01  2.87698642e-01  1.76227364e-01  3.24902177e-01\n",
            " -7.68531910e-01  1.35609241e-01  8.88454636e-02 -1.50163042e-01\n",
            "  6.69151839e-01 -1.56372490e-01 -2.28316014e-02 -2.50579740e-01\n",
            " -6.12145999e-01  5.23833366e-02  9.53970863e-02  1.60138708e-01\n",
            " -4.81950944e-02 -7.66668787e-02  3.50554243e-02 -6.39094469e-02\n",
            "  6.94568048e-01  2.72098894e-01 -2.76414192e-01 -7.56230802e-02\n",
            " -7.10966521e-02  2.91066194e-01 -1.00017818e-01 -1.30121286e-01\n",
            " -2.71549647e-01  2.71919252e-01 -6.14694695e-01  3.99068721e-01\n",
            " -4.13894219e-01  3.30272190e-01 -4.04201040e-01 -5.82818883e-02\n",
            " -6.21975103e-01 -1.64741840e-01 -3.88529129e-02  1.79940211e-01\n",
            " -3.11327737e-01  4.38726939e-01 -4.40242950e-01 -4.06706770e-01\n",
            " -4.55554089e-01  7.27881192e-02  1.90704018e-01 -3.93113913e-01\n",
            " -3.06661464e-01  5.74711667e-02  1.26143306e+00  8.04439713e-02\n",
            " -1.96617835e-01  7.97229210e-03  4.14550398e-02  7.55979508e-02\n",
            "  4.61029061e-01 -1.88420158e-01 -3.69124429e-01  1.57219820e-01\n",
            " -2.01983020e-01  1.31875290e-02  4.79814228e-01 -1.39836895e-01\n",
            " -3.85527254e-01  3.36521909e-01  3.99353771e-01  1.18889127e-01\n",
            " -7.65023154e-02 -4.84946312e-01 -7.60506851e-02  3.51754142e-01\n",
            " -4.39896969e-01  1.30901326e-01 -3.69662060e-01 -2.80100033e-01\n",
            " -3.44735786e-02 -3.39621207e-01  1.21174026e-01 -1.52412416e-01\n",
            " -4.78140798e-01 -2.35436293e-01 -2.13303769e-01  1.41488568e-01\n",
            "  2.07433108e-01  6.13845640e-01  1.76319568e-01  7.30660067e-02\n",
            " -5.53186113e-01 -4.25735953e-01  4.96368024e-01 -3.08417199e-01\n",
            " -3.01514441e-01  3.90052928e-01 -1.22048392e-01  2.48220338e-01\n",
            "  7.55592144e-02 -4.28806420e-02 -7.12682757e-01  7.20760336e-03\n",
            " -4.79111569e-02 -4.21607739e-02  1.08339252e-01 -5.62287735e-01\n",
            "  5.57332662e-01 -5.81189200e-01  2.23333301e-01 -3.40342244e-01\n",
            "  1.43484625e-01 -3.21710604e-01 -1.85776362e-01 -1.91863734e-01\n",
            "  2.56837717e-01 -3.84699395e-01  6.52944604e-02  2.44826727e-01\n",
            "  2.77855586e-02 -1.34455930e-01 -1.40680393e-01  1.58888810e-01\n",
            "  5.53061326e-01 -8.47434496e-01 -2.90962084e-02 -3.77173245e-01\n",
            "  2.79129142e-01 -2.63190764e-01  4.61421569e-01  5.17215488e-01\n",
            "  2.79590039e-01  1.46605840e-01  4.49167028e-02  4.04636281e-02\n",
            " -2.01356423e-01  4.65264457e-01 -4.19690815e-01 -1.30620263e-01\n",
            " -2.06781233e-01  1.66304435e-01  2.68592657e-01 -7.12193650e-01\n",
            " -2.61781330e-01  4.19228983e-01  5.93175040e-01 -1.00951589e-01]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCWJ90pD8r4a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}